splitSets(non.dtm)
fightinW=function(x,y){
outs=list()
# from FightinW in class5
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
# from FightinW in class5
rate.1=sum.1/sum(sum.1)
rate.0=sum.0/sum(sum.0)
# from dictionW() code
var.1=rate.1*(1-rate.1)
var.0=rate.0*(1-rate.0)
#odds of word use
odds.1=rate.1/(1-rate.1)
odds.0=rate.0/(1-rate.0)
outs[[3]]=log(odds.1)-log(odds.0)
outs[[3]][which(rate.1==0 | rate.0==0)]=NA
#dirichlet weights (flattish weights)
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
# from dictionW() code
w=(dirch.1-dirch.0)
return(w)
}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
#numi qs: am i using right data + do i need to filter to only y=1?
w.train=fightinW(x=x.train,y=y.train)
top.non=sort(abs(w.train),decreasing=F)[1:5]
bottom.non=sort(abs(w.train),decreasing=T)[1:5]
top.non
print(top.non)
print(bottom.non)
splitSets(non.dtm) # to get test, train, predict global variables
prC=table(y.train)/length(y.train)
Tj=rbind(
colSums(x.train[which(y.train==-0),]),
colSums(x.train[which(y.train==1),])
)
#  - unique occurances, w/ laplace smoother
Uj=rbind(
colSums(x.train[which(y.train==-0),]>0),
colSums(x.train[which(y.train==1),]>0)
)
M=ncol(Tj)
St=rowSums(Tj)
Su=rowSums(Uj)
PrT=Tj
PrT[1,]=(1+Tj[1,])/(M+St[1])
PrT[2,]=(1+Tj[2,])/(M+St[2])
PrU=Uj
PrU[1,]=(1+Uj[1,])/(M+Su[1])
PrU[2,]=(1+Uj[2,])/(M+Su[2])
# ordered classes
classes=c('Madison','Hamilton')
# document probability in train, test set
nbPredict=function(x,prior,likelihood,classes){
whichMax=function(Lk){
return(which(Lk==max(Lk)))
}
kclass=length(prior)
if(kclass!=nrow(likelihood)){
break('prior and liklihood have inconsistent number of classes')
}
cat(paste('###################\npredicting ',kclass,' classes\n###################',sep=''))
Lk=matrix(NA,nrow(x),kclass)
for(j in 1:nrow(x)){
for(ks in 1:kclass){
Lk[j,ks]=log(prior[ks])+sum(log(x[j,which(x[j,]>0)]*likelihood[ks,which(x[j,]>0)]))
}
}
#return(list('classes'=classes[apply(Lk,1,whichMax)],'loglik'=Lk))
return(classes[apply(Lk,1,whichMax)])
}
pred1=nbPredict(x=x.train,prior=prC,likelihood=PrT,classes=classes)
pred2=nbPredict(x=x.train,prior=prC,likelihood=PrU,classes=classes)
pred1[which(pred1=='Madison')]=0
pred1[which(pred1=='Hamilton')]=1
pred2[which(pred2=='Madison')]=0
pred2[which(pred2=='Hamilton')]=1
#good classification : esp total counts
table(pred1,y.train)
#good classification w/ unique counts
table(pred2,y.train)
test1=nbPredict(x=x.test,prior=prC,likelihood=PrT,classes=classes)
test2=nbPredict(x=x.test,prior=prC,likelihood=PrU,classes=classes)
test1[which(test1=='Madison')]=0
test1[which(test1=='Hamilton')]=1
test2[which(test2=='Madison')]=0
test2[which(test2=='Hamilton')]=1
table(test1,y.test)
table(test2,y.test)
nonstop_results
nonstop.prediction
pred2
as.numeric(strsplit(pred2, " ")[[1]])
pred2
str(pred2)
as.numeric(pred2)
apr_general <- function(test.predict, test.real){
add.vector <- test.predict + test.real
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.real - test.predict
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
apr_general(test1, y.test)
test1
y.test
apr_general(as.numeric(test1), y.test)
test.predict <- as.numeric(test1)
test.real <- y.test
add.vector <- test.predict + test.real
add.vector
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.real - test.predict
sub.vector
test.real
test/predict
test.predict
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
false_hamilton
false_madison
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
accuracy
precision_madison <- true_madison / (true_madison + false_madison)
precision_madison
true_madison
false_madison
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
precision_hamilton
test.predict
apr_general <- function(test.predict, test.real){
add.vector <- test.predict + test.real
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.real - test.predict
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
apr_general(test1, y.test)
apr_general(as.numeric(test1), y.test)
fightingW_results
outs=naiveBayes(y=y.train,x=x.train,laplace=1)
test.pred=predict(outs,x.test)
test.pred
outs
test.pred
table(test.pred,y.test)
y.test
test.pred
y.test
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warnings = FALSE, message = FALSE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
set.seed(1005)
load(file = "pset2/federalists.Rdata")
# using stop words from stopword() package
stops <- stopwords(kind = "en")
# note apostrophes are not removed in stops, but are in our texts
stops <- c(gsub(stops, pattern="[']", replace=''))
# make corpus
texts=VCorpus(VectorSource(papers[,1]))
# make dtm
dtm = as.matrix(DocumentTermMatrix(texts))
# remove non stop words
non.stops=array(TRUE,ncol(dtm))
stops=sort(stops)
col.names=colnames(dtm)
for(j in 1:length(stops)){
ik=which(stops[j]==col.names)
if(length(ik)>0){
non.stops[ik]=F
}
}
non.dtm  = dtm[,non.stops] # non-stop words
stop.dtm = dtm[,!non.stops] # stop words
# get row IDs for known and disputed papers
papers <- as.data.frame(papers)
knownpapers_ids <- which(papers$classes == c("hamilton", "madison"), arr.ind=TRUE)
disputedpapers_ids <- which(papers$classes == "disputed", arr.ind=TRUE)
# split papers into known and disputed
knownpapers <- papers[knownpapers_ids,]
disputedpapers <- papers[disputedpapers_ids,]
# recode known papers into 0 (madison) and 1 (hamilton)
knownpapers <- knownpapers %>%
mutate(classes = case_when(classes=="hamilton" ~ 1,
classes=="madison" ~ 0))
# function to split into testing, training, prediction sets
splitSets <- function(dtm){
# split dtm into known and disputed
dtm.known <- dtm[knownpapers_ids,]
dtm.disputed <- dtm[disputedpapers_ids,]
# define training, testing, and prediction sets
# randomly assign known papers to training (50%) and testing (50%)
N=nrow(dtm.known)
s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
# train with 1/2 of known
x.train <<- dtm.known[which(s.vec==1),]
y.train <<- knownpapers$classes[which(s.vec==1)]
# test with other 1/2 of known
x.test <<- dtm.known[which(s.vec==2),]
y.test <<- knownpapers$classes[which(s.vec==2)]
# predicted disputed
x.pred <<- dtm.disputed
#y.pred is unknown
}
splitSets(stop.dtm)
# function to weight words
dictionW=function(x,y){
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
rate.1=sum.1/(sum.0+sum.1)
rate.0=sum.0/(sum.0+sum.1)
var.1=rate.1*(1-rate.1)
var.0=rate.0*(1-rate.0)
# degenerate words get 0 weight
rate.1[which(sum.1==0)]=0
rate.0[which(sum.0==0)]=0
var.1[which(sum.1==0)]=1
var.0[which(sum.0==0)]=1
w=(rate.1-rate.0)/(var.1+var.0)
return(w)
}
# trim values to test
cutoffValues <- c(0.1, 0.2, 0.3, 0.5)
ttp <- function(cutoff, method){
# produce a predictor score in training set
# for 1a, 1b
if(method=="normal"){
w.train <- dictionW(x=x.train,y=y.train)
}
# for dirch weights in 1c
if(method=="dirch"){
w.train <- fightinW(x=x.train,y=y.train)
}
# test different cutoffs for trimming (TODO: IS THIS OK?)
w.train <- w.train[which(abs(w.train)<cutoff)]
z.train=x.train
for(j in 1:length(w.train)){
z.train[,j]=w.train[j]*x.train[,j]
}
Z.train=rowSums(z.train)
train.prediction=as.numeric(Z.train>mean(Z.train))
# testing set
z.test=x.test
for(j in 1:length(w.train)){
z.test[,j]=w.train[j]*x.test[,j]
}
Z.test=rowSums(z.test)
test.prediction=as.numeric(Z.test>mean(Z.test))
# predicting disputed papers
z.pred=x.pred
for(j in 1:length(w.train)){
z.pred[,j]=w.train[j]*x.pred[,j]
}
Z.pred=rowSums(z.pred)
pred.prediction=as.numeric(Z.pred>mean(Z.pred))
# we want test.prediction and pred.prediction for future analysis
predictions <- vector(mode = "list", length = 2)
predictions[[1]] <- unlist(test.prediction)
predictions[[2]] <- unlist(pred.prediction)
return(predictions)
}
testTrim <- function(method) {
# empty dataframe to store predictions
cutoffPredictions <- data.frame(matrix(ncol = 3, nrow = length(cutoffValues)))
names(cutoffPredictions) <- c("cutoff", "test.prediction", "pred.prediction")
# try different cutoffs
for(i in 1:length(cutoffValues)){
thisPrediction <- ttp(cutoffValues[[i]], method)
# note have to convert data type from vector to string to store in data frame
cutoffPredictions[i, ] <- c(cutoffValues[[i]], paste(thisPrediction[[1]], collapse=" "), paste(thisPrediction[[2]], collapse=" "))
}
# store predictions in global variable
cutoffPredictions <<- cutoffPredictions
}
testTrim("normal")
# function to calculate accuracy, precision, recall
apr <- function(cutoff, test.prediction){
add.vector <- test.prediction + y.train
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.prediction - y.train
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
runapr <- function(){
# empty list to store results
results <- vector(mode = "list", length = length(cutoffValues))
# run apr on each of the tests for the cutoff values
for(i in 1:length(cutoffValues)){
test.prediction <<- as.numeric(strsplit(cutoffPredictions$test.prediction[i], " ")[[1]])
results[[i]] <- apr(cutoffValues[[i]], test.prediction)
}
resultsDf <- as.data.frame(rbind(results[[1]], results[[2]], results[[3]], results[[4]])) # NOTE: will need to modified if number of cutoffs tested is modified
colnames(resultsDf) <- c("accuracy", "precision_madison", "precision_hamilton", "recall_madison", "recall_hamilton")
resultsDf <<- round(resultsDf, 2)
}
runapr()
print(resultsDf)
stop.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
stop_results <- as.numeric(resultsDf[1, ])
stop.prediction
length(stop.prediction)
splitSets(non.dtm)
testTrim("normal")
runapr()
print(resultsDf)
nonstop.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
nonstop_results <- as.numeric(resultsDf[1, ])
length(nonstop.prediction)
# fighting words weight function
splitSets(non.dtm)
fightinW=function(x,y){
outs=list()
# from FightinW in class5
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
# from FightinW in class5
rate.1=sum.1/sum(sum.1)
rate.0=sum.0/sum(sum.0)
# from dictionW() code
var.1=rate.1*(1-rate.1)
var.0=rate.0*(1-rate.0)
#odds of word use
odds.1=rate.1/(1-rate.1)
odds.0=rate.0/(1-rate.0)
outs[[3]]=log(odds.1)-log(odds.0)
outs[[3]][which(rate.1==0 | rate.0==0)]=NA
#dirichlet weights (flattish weights)
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
# from dictionW() code
w=(dirch.1-dirch.0)
return(w)
}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
length(fightingW.prediction)
#numi qs: am i using right data + do i need to filter to only y=1?
w.train=fightinW(x=x.train,y=y.train)
top.non=sort(abs(w.train),decreasing=F)[1:5]
bottom.non=sort(abs(w.train),decreasing=T)[1:5]
print(top.non)
print(bottom.non)
splitSets(non.dtm) # to get test, train, predict global variables
# use naive bayes, laplace smoothing class6 code to write function
# prior probability pr(C=Cj)
prC=table(y.train)/length(y.train)
# conditional probabilities
# operate at # of unique occurrances or total occurances
#  - total occurances, w/ laplace smoother
Tj=rbind(
colSums(x.train[which(y.train==-0),]),
colSums(x.train[which(y.train==1),])
)
#  - unique occurances, w/ laplace smoother
Uj=rbind(
colSums(x.train[which(y.train==-0),]>0),
colSums(x.train[which(y.train==1),]>0)
)
M=ncol(Tj)
St=rowSums(Tj)
Su=rowSums(Uj)
PrT=Tj
PrT[1,]=(1+Tj[1,])/(M+St[1])
PrT[2,]=(1+Tj[2,])/(M+St[2])
PrU=Uj
PrU[1,]=(1+Uj[1,])/(M+Su[1])
PrU[2,]=(1+Uj[2,])/(M+Su[2])
# ordered classes
classes=c('Madison','Hamilton')
# document probability in train, test set
nbPredict=function(x,prior,likelihood,classes){
whichMax=function(Lk){
return(which(Lk==max(Lk)))
}
kclass=length(prior)
if(kclass!=nrow(likelihood)){
break('prior and liklihood have inconsistent number of classes')
}
cat(paste('###################\npredicting ',kclass,' classes\n###################',sep=''))
Lk=matrix(NA,nrow(x),kclass)
for(j in 1:nrow(x)){
for(ks in 1:kclass){
Lk[j,ks]=log(prior[ks])+sum(log(x[j,which(x[j,]>0)]*likelihood[ks,which(x[j,]>0)]))
}
}
#return(list('classes'=classes[apply(Lk,1,whichMax)],'loglik'=Lk))
return(classes[apply(Lk,1,whichMax)])
}
pred1=nbPredict(x=x.train,prior=prC,likelihood=PrT,classes=classes)
pred2=nbPredict(x=x.train,prior=prC,likelihood=PrU,classes=classes)
pred1[which(pred1=='Madison')]=0
pred1[which(pred1=='Hamilton')]=1
pred2[which(pred2=='Madison')]=0
pred2[which(pred2=='Hamilton')]=1
table(pred1,y.train)
table(pred2,y.train)
bayes.prediction <- as.numeric(pred1)
length(bayes.prediction)
pred1
pred2
# try in the testing set
test1=nbPredict(x=x.test,prior=prC,likelihood=PrT,classes=classes)
test2=nbPredict(x=x.test,prior=prC,likelihood=PrU,classes=classes)
test1[which(test1=='Madison')]=0
test1[which(test1=='Hamilton')]=1
test2[which(test2=='Madison')]=0
test2[which(test2=='Hamilton')]=1
#good classification : esp total counts
table(test1,y.test)
#good classification w/ unique counts
table(test2,y.test)
apr_general <- function(test.predict, test.real){
add.vector <- test.predict + test.real
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.real - test.predict
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
bayes_results <- apr_general(as.numeric(test1), y.test)
bayes_results
