thisPrediction <- ttp(cutoffValues[[i]], method)
# note have to convert data type from vector to string to store in data frame
cutoffPredictions[i, ] <- c(cutoffValues[[i]], paste(thisPrediction[[1]], collapse=" "), paste(thisPrediction[[2]], collapse=" "))
}
# store predictions in global variable
cutoffPredictions <<- cutoffPredictions
}
testTrim("normal")
# function to calculate accuracy, precision, recall
apr <- function(cutoff, test.prediction){
add.vector <- test.prediction + y.train
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.prediction - y.train
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
runapr <- function(){
# empty list to store results
results <- vector(mode = "list", length = length(cutoffValues))
# run apr on each of the tests for the cutoff values
for(i in 1:length(cutoffValues)){
test.prediction <<- as.numeric(strsplit(cutoffPredictions$test.prediction[i], " ")[[1]])
results[[i]] <- apr(cutoffValues[[i]], test.prediction)
}
resultsDf <- as.data.frame(rbind(results[[1]], results[[2]], results[[3]], results[[4]])) # NOTE: will need to modified if number of cutoffs tested is modified
colnames(resultsDf) <- c("accuracy", "precision_madison", "precision_hamilton", "recall_madison", "recall_hamilton")
resultsDf <<- round(resultsDf, 2)
}
runapr()
print(resultsDf)
# fighting words weight function
splitSets(non.dtm)
fightinW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
#dirch weights
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
w=(dirch.1-dirch.0)
return(w)
}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
# from FightinW in class5
#rate.1=sum.1/sum(sum.1) #hamilton rate
#rate.0=sum.0/sum(sum.0) #madison
#adding in dirch weights REDUNDANT?
#rate.1=dirch.1/sum(dirch.1) #hamilton rate
# rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
words
fightingW.prediction
fightingW_results
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton + dirch
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton + dirch
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
words
scores
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
rate.1 = dirch.1/sum(dirch.1)
rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=rate.1/(1-rate.1) #odds of word in hamilton + dirch
odds.0=rate.0/(1-rate.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
#  rate.1 = dirch.1/sum(dirch.1)
# rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=rate.1/(1-rate.1) #odds of word in hamilton + dirch
odds.0=rate.0/(1-rate.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.1=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
#  rate.1 = dirch.1/sum(dirch.1)
# rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton + dirch
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.1=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
rate.1 = dirch.1/sum(dirch.1)
rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=rate.1/(1-rate.1) #odds of word in hamilton + dirch
odds.0=rate.0/(1-rate.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
rate.1 = dirch.1/sum(dirch.1)
rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=rate.1/(1-rate.1) #odds of word in hamilton + dirch
odds.0=rate.0/(1-rate.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
#rate.1 = dirch.1/sum(dirch.1)
# rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton + dirch
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
# fighting words weight function
splitSets(non.dtm)
fightinW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
#dirch weights
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
w=(dirch.1-dirch.0)
return(w)
}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#odds of word use
odds.1=dirch.1/(1-dirch.1) #odds of word in hamilton + dirch
odds.0=dirch.0/(1-dirch.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
# fighting words weight function
splitSets(non.dtm)
fightinW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),])
sum.0=colSums(x[which(y==0),])
#dirch weights
a.1=sum.1+2
a.0=sum.0+2
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
w=(dirch.1-dirch.0)
return(w)
}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
splitSets(non.dtm)
hamiltonW=function(x,y){
outs=list()
sum.1=colSums(x[which(y==1),]) #hamilton
sum.0=colSums(x[which(y==0),]) #madison
#dirchlet weights
a.1=sum.1+2 #laplace [2]
a.0=sum.0+2 #laplace [2]
dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0))
#word rates
rate.1 = dirch.1/sum(dirch.1)
rate.0=dirch.0/sum(dirch.0)
#odds of word use
odds.1=rate.1/(1-rate.1) #odds of word in hamilton + dirch
odds.0=rate.0/(1-rate.0) #odds of word in madison
sj=log(odds.1)-log(odds.0) #sj
return(sj)
}
scores <- hamiltonW(x.train, y.train)
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
#top five words using fightinW
w.train=fightinW(x=x.train,y=y.train)
top.fw <- names(sort(abs(w.train),decreasing=F)[1:5])
bottom.fw <- names(sort(abs(w.train),decreasing=T)[1:5])
#top five using hamilton disc score
top.ham <- names(sort(abs(scores),decreasing=F)[1:5])
bottom.ham <- names(sort(abs(scores),decreasing=T)[1:5])
words <- data.frame(top.fw,bottom.fw, top.ham, bottom.ham)
words
splitSets(non.dtm) # to get test, train, predict global variables
# use naive bayes, laplace smoothing class6 code to write function
# prior probability pr(C=Cj)
prC=table(y.train)/length(y.train)
# conditional probabilities
# operate at # of unique occurrances or total occurances
#  - total occurances, w/ laplace smoother
Tj=rbind(
colSums(x.train[which(y.train==-0),]),
colSums(x.train[which(y.train==1),])
)
#  - unique occurances, w/ laplace smoother
Uj=rbind(
colSums(x.train[which(y.train==-0),]>0),
colSums(x.train[which(y.train==1),]>0)
)
M=ncol(Tj)
St=rowSums(Tj)
Su=rowSums(Uj)
PrT=Tj
PrT[1,]=(1+Tj[1,])/(M+St[1])
PrT[2,]=(1+Tj[2,])/(M+St[2])
PrU=Uj
PrU[1,]=(1+Uj[1,])/(M+Su[1])
PrU[2,]=(1+Uj[2,])/(M+Su[2])
# ordered classes
classes=c('Madison','Hamilton')
# document probability in train, test set
nbPredict=function(x,prior,likelihood,classes){
whichMax=function(Lk){
return(which(Lk==max(Lk)))
}
kclass=length(prior)
if(kclass!=nrow(likelihood)){
break('prior and liklihood have inconsistent number of classes')
}
cat(paste('###################\npredicting ',kclass,' classes\n###################',sep=''))
Lk=matrix(NA,nrow(x),kclass)
for(j in 1:nrow(x)){
for(ks in 1:kclass){
Lk[j,ks]=log(prior[ks])+sum(log(x[j,which(x[j,]>0)]*likelihood[ks,which(x[j,]>0)]))
}
}
#return(list('classes'=classes[apply(Lk,1,whichMax)],'loglik'=Lk))
return(classes[apply(Lk,1,whichMax)])
}
# try in the testing set
test1=nbPredict(x=x.test,prior=prC,likelihood=PrT,classes=classes)
test2=nbPredict(x=x.test,prior=prC,likelihood=PrU,classes=classes)
test1[which(test1=='Madison')]=0
test1[which(test1=='Hamilton')]=1
test2[which(test2=='Madison')]=0
test2[which(test2=='Hamilton')]=1
# function to calculate accuracy, precision, recall
apr_general <- function(test.predict, test.real){
add.vector <- test.predict + test.real
# 2's indicate truly H, 0's indicate truly M
true_hamilton <- sum(add.vector==2)
true_madison <- sum(add.vector==0)
sub.vector <- test.real - test.predict
# +1 indicates classified as H when M, -1 indicates classified as M when H
false_hamilton <- sum(add.vector==1)
false_madison <- sum(add.vector==-1)
# overall accuracy
accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
# precision for madison
precision_madison <- true_madison / (true_madison + false_madison)
# precision for hamilton
precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
# recall for madison
recall_madison <- true_madison / (true_madison + false_hamilton)
# recall for hamilton
recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
# compiled results
results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
return(results)
}
# run apr on each of the tests
test1results <- apr_general(as.numeric(test1), y.test)
test2results <- apr_general(as.numeric(test2), y.test)
resultsDf <- as.data.frame(rbind(test1results, test2results))
colnames(resultsDf) <- c("accuracy", "precision_madison", "precision_hamilton", "recall_madison", "recall_hamilton")
resultsDf <<- round(resultsDf, 2)
bayesPrT_results <- apr_general(as.numeric(test1), y.test)
bayesPrU_results <- apr_general(as.numeric(test2), y.test)
pred1=nbPredict(x=x.pred,prior=prC,likelihood=PrT,classes=classes)
pred2=nbPredict(x=x.pred,prior=prC,likelihood=PrU,classes=classes)
pred1[which(pred1=='Madison')]=0
pred1[which(pred1=='Hamilton')]=1
pred2[which(pred2=='Madison')]=0
pred2[which(pred2=='Hamilton')]=1
bayesPrT.prediction <- as.numeric(pred1)
bayesPrU.prediction <- as.numeric(pred2)
# econ package for naivebayes implementation
outs=naiveBayes(y=y.train,x=x.train,laplace=1) # train
test.pred=predict(outs,x.test) # test
naivebayes_results <- apr_general(as.numeric(levels(test.pred))[test.pred], y.test) # validate
unknown.pred=predict(outs,x.pred) # predict
naivebayes.prediction <- as.numeric(levels(unknown.pred))[unknown.pred]
classMat <- rbind(stop.prediction, nonstop.prediction, fightingW.prediction, bayesPrT.prediction, bayesPrU.prediction, naivebayes.prediction)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(tidyverse)
library(lda)
library(stm)
load("pset3/adData.Rdata")
dat <- textProcessor(ads$texts, metadata = ads)
dat
