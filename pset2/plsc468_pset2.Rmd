---
title: "PLSC 468 Pset 2"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "3/11/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warnings = FALSE, message = FALSE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
```

# Problem 1: Supervised Learning to Classify Federalist Papers

*Objective: classify 15 disputed Federalist papers as written by either Madison (H = 0) or Hamilton (H = 1)*

*Data: matrix "papers" with 85 "papers" and "classes;" 51 known-Hamilton, 5 known-Jay, 14 known-Madison*

*Process: Split 65 known Madison and Hamilton papers into training and testing sets (use discretion on proportion but consider overfitting vs validation tradeoffs); Calibrate method on training set; Evaluate method on training set (recording relevant validation measures); Classify missing papers*

*Output: 4x5 validation table with rows for each of the classification approaches (a) stop-words, b) non-stopwords, c) fightin' words weights, d) naive bayes) and columns for 1) overall accuracy, 2) precision for Madison, 3) precision for Hamilton, 4) recall for Madison, and 5) recall for Hamilton.*

Setting the seed for future randomization:

```{r}
set.seed(1005)
```

Loading papers from .rda file:

```{r}
load(file = "pset2/federalists.Rdata")
```

## a) Mosteller and Wallace stopword dictionary approach (Kelly, done)

*Calculate word weights only using stopwords in known Federalist papers*

*Consider appropriate document-scores threshold to classify documents and trim word weights appropriately*

*What is your classification?*

First, we break up our document term matrix into two separate dtm's: stop words (stop.dtm, for use in 1a) and non-stop words (non.dtm, for use in 1b). We use the stop words from the stopword package.

```{r}
# using stop words from stopword() package
stops <- stopwords(kind = "en")

# note apostrophes are not removed in stops, but are in our texts
stops <- c(gsub(stops, pattern="[']", replace=''))

# make corpus
texts=VCorpus(VectorSource(papers[,1]))

# make dtm
dtm = as.matrix(DocumentTermMatrix(texts))

# remove non stop words

non.stops=array(TRUE,ncol(dtm))

stops=sort(stops)

col.names=colnames(dtm)
for(j in 1:length(stops)){
	ik=which(stops[j]==col.names)
	if(length(ik)>0){
		non.stops[ik]=F
	}
}

non.dtm  = dtm[,non.stops] # non-stop words
stop.dtm = dtm[,!non.stops] # stop words
```

Second, we break up the papers into known (knownpapers) and disputed (disputedpapers) papers. We also recode the classes into "hamilton" = 1 and "madison" = 0.

```{r}
# get row IDs for known and disputed papers
papers <- as.data.frame(papers)
knownpapers_ids <- which(papers$classes == c("hamilton", "madison"), arr.ind=TRUE)
disputedpapers_ids <- which(papers$classes == "disputed", arr.ind=TRUE)

# split papers into known and disputed
knownpapers <- papers[knownpapers_ids,]
disputedpapers <- papers[disputedpapers_ids,]

# recode known papers into 0 (madison) and 1 (hamilton)
knownpapers <- knownpapers %>%
  mutate(classes = case_when(classes=="hamilton" ~ 1,
                   classes=="madison" ~ 0))

```

Now, we work with the stop words dtm to make training (50% of known papers), testing (other 50% of known papers), and prediction (all disputed papers) sets. We make this into the function splitSets() that can be used elsewhere in the pset and also, due to function scope, ensure that all variables needed later on (x.train, y.train, x.test, y.test, x.pred) are stored as global variables using the operator <<-.

```{r}
# function to split into testing, training, prediction sets
splitSets <- function(dtm){
  # split dtm into known and disputed
  dtm.known <- dtm[knownpapers_ids,]
  dtm.disputed <- dtm[disputedpapers_ids,]
  
  # define training, testing, and prediction sets
  
  # randomly assign known papers to training (50%) and testing (50%)
  N=nrow(dtm.known)
  s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
  
  # train with 1/2 of known
  x.train <<- dtm.known[which(s.vec==1),]
  y.train <<- knownpapers$classes[which(s.vec==1)]
  
  # test with other 1/2 of known
  x.test <<- dtm.known[which(s.vec==2),]
  y.test <<- knownpapers$classes[which(s.vec==2)]
  
  # predicted disputed
  x.pred <<- dtm.disputed
  #y.pred is unknown
}

splitSets(stop.dtm)
```

Next, we use a function to weight words called dictionW().

```{r}
# function to weight words
dictionW=function(x,y){
	sum.1=colSums(x[which(y==1),])
	sum.0=colSums(x[which(y==0),])

	rate.1=sum.1/(sum.0+sum.1)
	rate.0=sum.0/(sum.0+sum.1)
	var.1=rate.1*(1-rate.1)
	var.0=rate.0*(1-rate.0)

	# degenerate words get 0 weight
	rate.1[which(sum.1==0)]=0
	rate.0[which(sum.0==0)]=0
	var.1[which(sum.1==0)]=1
	var.0[which(sum.0==0)]=1

	w=(rate.1-rate.0)/(var.1+var.0)
	return(w)
}
```

The function ttp() carries out the training and predicts to which class the papers belong. To account for different trimming values, the function accepts different cutoffs as an input.

With the function testTrim(), we try cutoffs of 0.1, 0.2, 0.3, and 0.5 to try to attain the best model. The resulting testing and predicting results are stored in the global variable cutoffPredictions.

```{r}
# trim values to test
cutoffValues <- c(0.1, 0.2, 0.3, 0.5)

ttp <- function(cutoff, method){
  # produce a predictor score in training set

  # for 1a, 1b
  if(method=="normal"){
    w.train <- dictionW(x=x.train,y=y.train)
  }
  
  # for dirch weights in 1c
  if(method=="dirch"){
    w.train <- FightinW(x=x.train,y=y.train)
  }
  
  # test different cutoffs for trimming (TODO: IS THIS OK?)
  w.train <- w.train[which(abs(w.train)<cutoff)]
  
  z.train=x.train
  
  for(j in 1:length(w.train)){
  	z.train[,j]=w.train[j]*x.train[,j]
  }
  
  Z.train=rowSums(z.train)
  
  train.prediction=as.numeric(Z.train>mean(Z.train))
  
  # testing set
  
  z.test=x.test
  for(j in 1:length(w.train)){
  	z.test[,j]=w.train[j]*x.test[,j]
  }
  
  Z.test=rowSums(z.test)
  
  test.prediction=as.numeric(Z.test>mean(Z.test))
  
  # predicting disputed papers
  
  z.pred=x.pred
  for(j in 1:length(w.train)){
  	z.pred[,j]=w.train[j]*x.pred[,j]
  }
  
  Z.pred=rowSums(z.pred)
  
  pred.prediction=as.numeric(Z.pred>mean(Z.pred))
  
  # we want test.prediction and pred.prediction for future analysis
  predictions <- vector(mode = "list", length = 2)
  predictions[[1]] <- unlist(test.prediction)
  predictions[[2]] <- unlist(pred.prediction)

  return(predictions)
}

testTrim <- function(method) {
  # empty dataframe to store predictions
  cutoffPredictions <- data.frame(matrix(ncol = 3, nrow = length(cutoffValues)))
  names(cutoffPredictions) <- c("cutoff", "test.prediction", "pred.prediction")
 
  # try different cutoffs
   for(i in 1:length(cutoffValues)){
    thisPrediction <- ttp(cutoffValues[[i]], method)
    # note have to convert data type from vector to string to store in data frame
    cutoffPredictions[i, ] <- c(cutoffValues[[i]], paste(thisPrediction[[1]], collapse=" "), paste(thisPrediction[[2]], collapse=" "))
   }
  
  # store predictions in global variable
  cutoffPredictions <<- cutoffPredictions
}

testTrim("normal")
```

The global variable cutoffPredictions stores our test.prediction and pred.prediction at various cutoff values, where 1 represents Hamilton authorship and 0 represents Madison authorship

To determine which set of predictions are the best, we assess the performance of the model by calculating accuracy, precision, and recall for the predicted values in the testing set versus the actual values. We create the function apr() to do so through this report based on true positives, true negatives, false positives, and false negatives. We run the function apr() on all of the cutoff values by using the function runapr() and store results in the resultsDf variable.

```{r}
# function to calculate accuracy, precision, recall

apr <- function(cutoff, test.prediction){
  
  add.vector <- test.prediction + y.train
  # 2's indicate truly H, 0's indicate truly M
  
  true_hamilton <- sum(add.vector==2)
  true_madison <- sum(add.vector==0)
  
  sub.vector <- y.train - test.prediction
  # +1 indicates classified as H when M, -1 indicates classified as M when H
  
  false_hamilton <- sum(add.vector==1)
  false_madison <- sum(add.vector==-1)
  
  # overall accuracy
  accuracy <- (true_hamilton + true_madison) / (true_hamilton + true_madison + false_hamilton + false_madison)
  
  # precision for madison
  precision_madison <- true_madison / (true_madison + false_madison)
  
  # precision for hamilton
  precision_hamilton <- true_hamilton / (true_hamilton + false_hamilton)
  
  # recall for madison
  recall_madison <- true_madison / (true_madison + false_hamilton)
  
  # recall for hamilton
  recall_hamilton <- true_hamilton / (true_hamilton + false_madison)
  
  # compiled results
  results <- c(accuracy, precision_madison, precision_hamilton, recall_madison, recall_hamilton)
  return(results)
}

runapr <- function(){
  # empty list to store results
  results <- vector(mode = "list", length = length(cutoffValues))
  
  # run apr on each of the tests for the cutoff values
  for(i in 1:length(cutoffValues)){
    test.prediction <<- as.numeric(strsplit(cutoffPredictions$test.prediction[i], " ")[[1]])
  
    results[[i]] <- apr(cutoffValues[[i]], test.prediction)
  }
  
  resultsDf <- as.data.frame(rbind(results[[1]], results[[2]], results[[3]], results[[4]])) # NOTE: will need to modified if number of cutoffs tested is modified
  colnames(resultsDf) <- c("accuracy", "precision_madison", "precision_hamilton", "recall_madison", "recall_hamilton")
  resultsDf <<- round(resultsDf, 2)
}

runapr()

print(resultsDf)
```

At different cutoff values, we note that the model gives the exact same classification, resulting in the same accuracy, precision, and recall. It is therefore arbitrary which cutoff we choose. We arbitrarily choose the results from the cutoff of 0.1 to compile into the comparative matrices at the end of this report, noting that the results are the exact same from any other cutoff.

```{r}
stop.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
stop_results <- as.numeric(resultsDf[1, ])
```

## b) Non-stopwords (Kelly, done)

*Replicate part a) using non-stopwords*

*Calculate weights using same word-weighting approach*

*Consider appropriate document-scores threshold to classify documents and trim word weights appropriately*

*What is your classification?*

Fortunately, we benefit from all of the functions created in 1a and can accomplish all of this in only 3 lines of code! We repeat the same techniques as in 1a, but use the document term matrix that does not include stopwords: non.dtm.

```{r}
splitSets(non.dtm)
testTrim("normal")
runapr()

print(resultsDf)
```

Again, we note, at different cutoff values, the model gives the exact same classification, resulting in the same accuracy, precision, and recall. It is therefore arbitrary which cutoff we choose. We arbitrarily choose the results from the cutoff of 0.1 to compile into the comparative matrices at the end of this report, noting that the results are the exact same from any other cutoff.

```{r}
nonstop.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
nonstop_results <- as.numeric(resultsDf[1, ])
```

## c) quasi-Bayesian Fightin' Words (Numi?)

*Use discretion over whether to include or discard stopwords or rare/frequent words*

We will only be using the non-stopwords in the analysis that follows.

*Produce Hamilton-discrimination score* (TODO: what are these?)

```{r}
# fighting words weight function 
splitSets(stop.dtm)

FightinW=function(x,y){
	# from FightinW in class5
  sum.1=colSums(x[which(y==1),])
	sum.0=colSums(x[which(y==0),])

	# from FightinW in class5
  rate.1=sum.1/sum(sum.1)
  rate.0=sum.0/sum(sum.0)
  
  # from dictionW() code
	var.1=rate.1*(1-rate.1)
	var.0=rate.0*(1-rate.0)
	
  #dirichlet weights (flattish weights)
  a.1=sum.1+1
  a.0=sum.0+1
  dirch.1=(sum.1+a.1)/(sum(sum.1)+sum(a.1))
  dirch.0=(sum.0+a.0)/(sum(sum.0)+sum(a.0)) 

  # from dictionW() code
	w=(dirch.1-dirch.0)/(var.1+var.0)
	return(w)
}
```

*What is your classification?*

```{r}
# same steps as before, changing weighting method to "dirch"
splitSets(non.dtm)
testTrim("dirch")
runapr()
print(resultsDf)
```

Again, we note, at different cutoff values, the model gives the exact same classification, resulting in the same accuracy, precision, and recall. It is therefore arbitrary which cutoff we choose. We arbitrarily choose the results from the cutoff of 0.1 to compile into the comparative matrices at the end of this report, noting that the results are the exact same from any other cutoff.

```{r}
fightingW.prediction <- as.numeric(strsplit(cutoffPredictions$pred.prediction[1], " ")[[1]])
fightingW_results <- as.numeric(resultsDf[1, ])
```

*Which 5 words are scored the most, and which five are the least Hamiltonian?* (TODO: what is going on here?)

```{r eval=F}
#Word frequency 
names(sort(abs(outs[[x]]),decreasing=T)[1:5])
names(sort(abs(outs[[x]]),decreasing=F)[1:5])


#to do: select top five highest and lowest 
```

## d) Naive Bayes (Chelsea?)

*Use discretion on whether to discard stopwords or other rare/frequent words*

We will only be using the non-stopwords in the analysis that follows.

*Write a function to perform Naive Bayes classification to predict missing author indicators*

*Make sure to use Laplace smoothing* (TODO)

```{r, eval = F}
splitSets(non.dtm) # to get test, train, predict global variables

# use naive bayes, laplace smoothing class6 code to write function
```

*What are your classifications?* (UPDATE NEEDED)

```{r, eval=F}
# previous steps (probably need to change)
testTrim("normal")
runapr()
print(resultsDf)
```

*Compare results to naiveBayes()* (FROM CLASS 6 CODE)

```{r, eval = F}
# econ package for naivebayes implementation
outs=naiveBayes(y=y.train,x=x.train,laplace=1)
test.pred=predict(outs,x.test)
1-mean(abs(as.numeric(as.character(test.pred))-y.test)/2)
table(test.pred,y.test)
```

## Matrix of Classifications (in progress)

```{r}
classMat <- rbind(stop.prediction, nonstop.prediction, fightingW.prediction)
classMat <- as.data.frame(classMat)
colnames(classMat) <- disputedpapers_ids
classMat
```

## Matrix of Results (in progress)

```{r}
resultsMat <- rbind(stop_results, nonstop_results, fightingW_results)
colnames(resultsMat) <- c("overall accuracy", "precision_madison", "precision_hamilton", "recall_madison", "recall_hamilton")
resultsMat <- as.data.frame(resultsMat)
resultsMat <- round(resultsMat, 2)
resultsMat
```