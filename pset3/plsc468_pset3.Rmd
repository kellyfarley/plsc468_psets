---
title: "PLSC 468 Pset 3"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/8/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(tidyverse)
library(stringr)
library(tm)
library(RWeka)
library(lda)
library(topicmodels) 
library(stm)
```
Setting the seed for future randomization:

```{r}
set.seed(1005)
```

Loading papers from .rda file:

```{r}
load(file = "pset3/adData.Rdata")
```

Goal: to estimate a series of unsupervised topic models using 3255 ads aired in 2008

# Part 1

**Goal: Choose an appropriate value to set for the number of topics *K* that will be estimated for each model**

*Our choice of K:*

*Description of method:*

Good reference for STM: https://sicss.io/2019/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html

Good reference for interpreting plots: https://juliasilge.com/blog/evaluating-stm/

We use the function textProcessor() to build our corpus. This function converts text to lowercase, removes punctuation, stopwords, and numbers, and stems words. We also use the function prepDocuments(), which eliminates very common and very rare terms. Our data is now prepped in the variable "outs" for future STM analysis.

```{r}
temp = textProcessor(documents = ads$texts, metadata = ads[, 2:7]) # clean data
outs = prepDocuments(temp$documents, temp$vocab, temp$meta) # eliminates very common and very rare terms
```

One of the most difficult decisions in STM modeling is choosing the best value of k for the data. We use the stm::searchK() function to try k's ranging from 2-20 to help find the best value. This line of code can take hours to run, so it is not evaluated in this code; rather, the results are provided in a plot. We first test a few sample points between k=5 and k=50, then narrow in on the k=10 to k=20 interval to decide on the best value of k.

```{r eval=F}
# determining best value of K
findingk <- searchK(outs$documents, outs$vocab, K = c(5, 10, 15, 20, 50),
 prevalence =~ party + tone, data = outs$meta, verbose=FALSE)

save(findingk, file = "pset3/stm_k_results.Rda")

pdf("pset3/findingkplot.pdf") 
plot(findingk)
dev.off()

findingk_narrowed <- searchK(outs$documents, outs$vocab, K = c(10:20),
 prevalence =~ party + tone, data = outs$meta, verbose=FALSE)

save(findingk_narrowed, file = "pset3/stm_k_results.Rda")
pdf("pset3/findingkplot_narrowed.pdf") 
plot(findingk_narrowed)
dev.off()

k = 11 # thoughts?
```

*Defense of method:*

*Preprocessing of ad strings into correct format for lda() and stm():*

```{r eval = F}
#collect some covars
#Numi tryng and failing to clean
party <- outs$party
state <- str_sub(ads$state)
df=as.data.frame(cbind(party,state),stringsAsFactors=F)
df[,1]=as.numeric(df[,1])
df[,2]=as.factor(df[,2])



#here's his og code 
year=str_sub(rownames(tdoc),1,4)
pty=pty
state=str_sub(rownames(tdoc),6,7)
platforms=as.data.frame(cbind(pty,year,state),stringsAsFactors=F)
platforms[,1]=as.numeric(platforms[,1])
platforms[,2]=as.factor(platforms[,2])
platforms[,3]=as.factor(platforms[,3])

# reproduce strings from word counts for use in lexicalize
strs=array(NA,nrow(documents))
for(i in 1:nrow(tdoc)){
	strs[i]=paste(names(which((tdoc[i,])>0)),collapse=' ')
}

platforms$texts=strs
```

```{r eval = F}
# 5. tuning lda to improve selection K
library(ldatuning)

result[[5]] <- FindTopicsNumber(
  xdm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
```

# Part 2

**Goal: Setting k = K, estimate the following topic models. For each model, produce a list of the top 10 most frequent works across each of the k topics**

## 2a: Unsupervised LDA model using lda::lda.collapsed.gibs.sampler(.., k) (Numi)

```{r eval = F}
# 1. baseline LDA
set.seed(1005)

# pre-processing step prior to lexicalizing in the text
#texts20=texts[iq] # retained from above, indexing those with at least 10 words

# lda package has a specific function used to count words: lexicalize
#  - lexicalize counts words in string array 'texts', and stores them in compact fashion
#  - compactness is achieved by ignored word counts when these are 0 (zero)
#  - side not, lexicalize also ignores 'counts' and just sets mentions to be 0,1

docs=lexicalize(vocab)
# you can extract the word list in the 2nd part of the docs list object
vocab=docs[[2]]
result=list()
# this is the workhorse function
k=10 # pre-specify # of topics
result[[1]]=lda.collapsed.gibbs.sampler(docs[[1]], 		## lexicalized word counts
	k,  			## k num of clusters
	vocab,		## vocab list in docs[[2]] #
	100,  ## Num iterations
	alpha = 0.1, # Note the alpha prior is pre-set; ideally would tune this using some validation
	eta = 0.1,   # Note the eta prior is pre-set; ideally would tune this using some validation
	compute.log.likelihood=TRUE
)

###################################################
lda_results <- top.topic.words(result[[1]]$topics,10)

```

## 2b: Supervised LDA model to predict *party* using lda::slda.em(..., party, k) (Numi)

```{r eval = F}
# 2. sLDA = inferring topics by predicting an outcome/annotation
docs=lexicalize(vocab)
# you can extract the word list in the 2nd part of the docs list object
vocab=docs[[2]]

# pre-set some parameters
k = 10
# starting values for prediction
params = sample(c(-1, 1), k, replace=TRUE)
alpha = 1.0
eta = 0.1

# annotation to predict
y=outs$party

result[[2]]=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, # gaussian outcomes when FALSe
		method="sLDA"
)

###################################################
slda_party_results <- top.topic.words(result[[2]]$topics,10)
```

## 2c: Supervised LDA model to predict *tone* using lda::slda.em(..., tone, k) (Numi)
```{r eval = F}
# 2. sLDA = inferring topics by predicting an outcome/annotation
# pre-set some parameters
set.seed(1005)
k = 10
# starting values for prediction
params = sample(c(-1, 1), k, replace=TRUE)
alpha = 1.0
eta = 0.1

# annotation to predict
y=outs$tone

result[[2]]=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, 
		method="sLDA"
)

###################################################
slda_tone_results <- top.topic.words(result[[2]]$topics,10)
```

## 2d: Structural topic model using stm::stm(..., prevalence = ~ party + tone, k) (Kelly)

In work done in part 1, we cleaned our data using textProcessor() and prepDocuments() and determined the best value of k, so we are ready to run an stm model.

```{r}
result_2d = stm(documents=outs$documents,vocab=outs$vocab,K=k,
	prevalence=~party+tone,data=outs$meta)
```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2d)
```

For this assignment, we are also interested in collecting the top 10 words in each category.

```{r}
# (less than parsimonious) interface for collecting top 10 words =>
stm_results_2d <- t(labelTopics(model=result_2d, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)
```

## 2e: Structural topic model using stm::stm(..., prevalence = ~ party + tone + office + policy + as.factor(state), k) (Kelly)

We repeat the same analysis as in 2d but use more variables for the modeling.

```{r}
result_2e = stm(documents=outs$documents,vocab=outs$vocab,K=k,
	prevalence=~party+tone+office+policy+as.factor(state),data=outs$meta)
```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2e)
```

For this assignment, we are also interested in collecting the top 10 words in each category.

```{r}
# (less than parsimonious) interface for collecting top 10 words =>
stm_results_2e <- t(labelTopics(model=result_2e, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)
```

## Reflection
**Are these roughly the same topics, or are there major differences in the topics being estimated?**
We report the top ten words for each model specification in the results matrix. 
```{r eval = F}
result_matrix <- rbind(lda_results, slda_party_results, slda_tone_results, stm_results_2d, stm_results_2e)
```

