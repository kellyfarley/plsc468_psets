---
title: "PLSC 468 Pset 3"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/8/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(tidyverse)
library(stringr)
library(tm)
library(RWeka)
library(lda)
library(topicmodels) 
library(stm)
library(ldatuning)
library(SnowballC)
```
Setting the seed for future randomization:

```{r}
set.seed(1005)
```

Loading papers from .rda file:

```{r}
load(file = "pset3/adData.Rdata")
```

Goal: to estimate a series of unsupervised topic models using 3255 ads aired in 2008

# Part 1

**Goal: Choose an appropriate value to set for the number of topics *K* that will be estimated for each model**

*Preprocessing of ad strings into correct format for lda() and stm():*
lda() cleaning 

First, we prepare the ads data by removing erroneous words that do not provide generative insight into topics generated by our model. We begin by stemming words, converting them to lowercase, then removing stop words. Because we are working with ads data, the names of candidates also appear frequently in our corpus. We make the decision not to remove these names. Our reasoning is that the identity of candidates is closely associated with a set of words this enables us to better understand what our model is potentially capturing when categorizing words into topics. This granualarity can help us differentiate subtle differennces in word groupings, for example the differences between words associated wiht Tea Party Republicans versus a classical GOP candidate.  
```{r eval = F}
#taking text data from ads
txts=ads$texts
#stemming words
txts=stemDocument(txts)
#changing to lowercase 
txts=tolower(txts)
#removing stopwords 
txts=removeWords(txts,stopwords("english"))
txts=stripWhitespace(txts)

#lexicalize strings for lda()
docs=lexicalize(txts)
```

stm() cleaning 

Next, we use the function textProcessor() to build our corpus for the stm() models. This function converts text to lowercase, removes punctuation, stopwords, and numbers, and stems words. We also use the function prepDocuments(), which eliminates very common and very rare terms. Our data is now prepped in the variable "outs_stm" for future STM analysis.
```{r eval = F}
ads$txts=txts
temp_stm = textProcessor(documents=ads$txts,metadata=ads[,2:7])
outs_stm = prepDocuments(temp_stm$documents, temp_stm$vocab, temp_stm$meta)
```

*Our choice of K:*

*Description of method:*
One of the most difficult decisions in STM modeling is choosing the best value of k for the data. We use the stm::searchK() function to try k's ranging from 2-20 to help find the best value. This line of code can take hours to run, so it is not evaluated in this code; rather, the results are provided in a plot. We first test a few sample points between k=5 and k=50, then narrow in on the k=10 to k=20 interval to decide on the best value of k.

```{r eval=F}
# determining best value of K
findingk <- searchK(outs_stm$documents, outs_stm$vocab, K = c(5, 10, 15, 20, 50),
 prevalence =~ party + tone, data = outs_stm$meta, verbose=FALSE)

save(findingk, file = "pset3/stm_k_results.Rda")

pdf("pset3/findingkplot.pdf") 
plot(findingk)
dev.off()

findingk_narrowed <- searchK(outs_stm$documents, outs_stm$vocab, K = c(10:20),
 prevalence =~ party + tone, data = outs_stm$meta, verbose=FALSE)

save(findingk_narrowed, file = "pset3/stm_k_results.Rda")
pdf("pset3/findingkplot_narrowed.pdf") 
plot(findingk_narrowed)
dev.off()

k = 11 # thoughts?
```

```{r}
# code from chelsea (done in the same way as class)

# creating a tdm of documents
docs = Corpus(VectorSource(outs_stm$documents))
tdm = DocumentTermMatrix(docs)

# calculating lda tuning metrics - big picture overview
metrics <- FindTopicsNumber(
  tdm,
  topics = seq(from = 2, to = 100, by = 20),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics)

# calculating lda tuning metrics - 5-30

metrics_final <- FindTopicsNumber(
  tdm,
  topics = seq(from = 5, to = 30, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics_final)

# think best is k=11
```

We want to minimize the Arun and CaoJuan metrics, and we want to maximize the Deveaud and Grifiths metrics. First, we calculated these metrics for a large K range (2-100) with large steps to observe the general pattern. It seems like the Arun, CaoJuan, and Deveaud metrics decrease over time, with the CaoJuan metric being the lowest around 22, the Griffiths metric increases over time. We then narrowed in on a K range from 5-30. From the plots, while the 4 metrics do not agree on a best K, it seems that 11 topics may be the most suitable when taking all metrics into account. While the Arun, CaoJuan, and Griffiths metrics are still moving, the Deveaud metric has a local peak. 

*Defense of method:*

*Preprocessing of ad strings into correct format for lda() and stm():*

```{r eval = F}
#here's his og code 
year=str_sub(rownames(tdoc),1,4)
pty=pty
state=str_sub(rownames(tdoc),6,7)
platforms=as.data.frame(cbind(pty,year,state),stringsAsFactors=F)
platforms[,1]=as.numeric(platforms[,1])
platforms[,2]=as.factor(platforms[,2])
platforms[,3]=as.factor(platforms[,3])

# reproduce strings from word counts for use in lexicalize
strs=array(NA,nrow(documents))
for(i in 1:nrow(tdoc)){
	strs[i]=paste(names(which((tdoc[i,])>0)),collapse=' ')
}

platforms$texts=strs
```

```{r eval = F}
# 5. tuning lda to improve selection K
library(ldatuning)

result[[5]] <- FindTopicsNumber(
  xdm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
```

# Part 2

**Goal: Setting k = K, estimate the following topic models. For each model, produce a list of the top 10 most frequent works across each of the k topics**

## 2a: Unsupervised LDA model using lda::lda.collapsed.gibs.sampler(.., k) (Numi)

Here we use the run the pre-processed string "txts" through the lexacalize fuction to enable use of the lda package. In this model we manually set k, alpha, and eta here for the remainder of lda() models. We define these model parameters as global variables that are then used for all lda models. We use the value of k determined above for the number of topics.

```{r eval = F}
#processing text for lda analysis 
vocab = docs[[2]]
result=list()

#preset parameters for lda() models
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)

#running the model!
result_2a=lda.collapsed.gibbs.sampler(docs[[1]], 		
	k,  			
	vocab,		
	100,  
	alpha = alpha, 
	eta = eta,   
	compute.log.likelihood=TRUE
)

```
Top Words
```{r}
lda_results <- top.topic.words(result_2a$topics,10)
colnames(lda_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(lda_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")

tibble(lda_results)
```

## 2b: Supervised LDA model to predict *party* using lda::slda.em(..., party, k) (Numi)
Next, we introduce party into our model. 

```{r eval = F}
# annotation to predict
y=ads$party

result_2b=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, 
		method="sLDA"
)
```
Top Words
```{r}
slda_party_results <- top.topic.words(result_2b$topics,10)

colnames(slda_party_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(slda_party_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
tibble(slda_party_results)
```

## 2c: Supervised LDA model to predict *tone* using lda::slda.em(..., tone, k) (Numi)
```{r eval = F}
# annotation to predict
y=ads$tone

result_2c=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, 
		method="sLDA"
)
```
Top Ten Words
```{r}
slda_tone_results <- top.topic.words(result_2c$topics,10)

colnames(slda_tone_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(slda_tone_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")

tibble(slda_tone_results)
```

## 2d: Structural topic model using stm::stm(..., prevalence = ~ party + tone, k) (Kelly)

In work done in part 1, we cleaned our data using textProcessor() and prepDocuments() and determined the best value of k, so we are ready to run an stm model.

```{r}
result_2d = stm(documents=outs_stm$documents,vocab=outs_stm$vocab,K=k,
	prevalence=~party+tone,data=outs_stm$meta)
```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2d)
```

For this assignment, we are also interested in collecting the top 10 words in each category.

```{r}
# (less than parsimonious) interface for collecting top 10 words =>
stm_results_2d <- t(labelTopics(model=result_2d, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results_2d) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(stm_results_2d) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
tibble(stm_results_2d)
```

## 2e: Structural topic model using stm::stm(..., prevalence = ~ party + tone + office + policy + as.factor(state), k) (Kelly)

We repeat the same analysis as in 2d but use more variables for the modeling.

```{r}
result_2e = stm(documents=outs_stm$documents,vocab=outs_stm$vocab,K=k,
	prevalence=~party+tone+office+policy+as.factor(state),data=outs_stm$meta)

```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2e)
```

For this assignment, we are also interested in collecting the top 10 words in each category.

```{r}
# (less than parsimonious) interface for collecting top 10 words =>
stm_results_2e <- t(labelTopics(model=result_2e, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)


colnames(stm_results_2e) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(stm_results_2e) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
tibble(stm_results_2e)
```

## Reflection
**Are these roughly the same topics, or are there major differences in the topics being estimated?**
We report the top ten words for each model specification. 

Looking first at LDA models, there does seem to be variation in the topics produced by each model. The basic LDA model does seem to group topics in a logical fashion. For example, Topic #1 seems to reflect some sort of pro-families pro-business sentiment whereas topic #4 seems to focus on energy and the domestic economy. 

Turning to the party prediction models, topics follow what how we might a priori group words based on partisanship. Topic #1 seems to reflect a main stream GOP platform, focusing on taxation, immigration, and families. In contrast, Topic #6 focuses on grassroots democratic rhetoric, mentioning workers, families, business, and the economy. There are a few topics that seem poorly defined such as Topic #2 which seems to simply have filler words that were not eliminated with stop words, Topic #4 seems to capture words associated with an Alaskan senate race, and Topic #9 also seems to just capture erroneous rhetoric from the ads. 

Interestingly, the tone prediction models generate very different results. In some sense, the groupings seem to just reflect how one would group words based on their plain meaning. For example, Topic 1 groups the words "energi", which in context presumably refers to energy policy, with words about "chang". The tone here seems to be a positive one. Similarly, Topic 9 seems to just group a series of words related to money. However, it is not completely clear what organizing principle is captured by each of the topics or what content is embodied in "tone". 
```{r eval = F}
lda_results
slda_party_results
slda_tone_results
stm_results_2d
stm_results_2e
```