---
title: "PLSC 468 Pset 3"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/8/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
# wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(tidyverse)
library(stringr)
library(tm)
library(lda)
library(topicmodels) 
library(stm)
library(ldatuning)
library(SnowballC)
```

Setting the seed for future randomization:

```{r}
set.seed(1005)
```

Loading papers from .rda file:

```{r}
load(file = "pset3/adData.Rdata")
```

Goal: to estimate a series of unsupervised topic models using 3255 ads aired in 2008

# Part 1

**Goal: Choose an appropriate value to set for the number of topics *K* that will be estimated for each model**

## Preprocessing of ad strings into correct format for lda() and stm():

### lda() cleaning 

First, we prepare the ads data by removing erroneous words that do not provide generative insight into topics generated by our model. We begin by stemming words, converting them to lowercase, then removing stop words. Because we are working with ads data, the names of candidates also appear frequently in our corpus. We make the decision not to remove these names. Our reasoning is that the identity of candidates is closely associated with a set of words this enables us to better understand what our model is potentially capturing when categorizing words into topics. This granularity can help us differentiate subtle differences in word groupings, for example the differences between words associated with Tea Party Republicans versus a classical GOP candidate. OUr data is now prepped in the variable "docs" for future LDA analysis.

```{r}
#taking text data from ads
txts=ads$texts
#stemming words
txts=stemDocument(txts)
#changing to lowercase 
txts=tolower(txts)
#removing stopwords 
txts=removeWords(txts,stopwords("english"))
txts=stripWhitespace(txts)
#remove special characters
txts=gsub("[[:punct:]]", "", txts)

#lexicalize strings for lda()
docs=lexicalize(txts)

# sort(unique(docs$vocab)) # proof that special characters are gone
```

### stm() cleaning 

Next, we use the function textProcessor() to build our corpus for the stm() models. This function converts text to lowercase, removes punctuation, stopwords, and numbers, and stems words. We also use the function prepDocuments(), which eliminates very common and very rare terms. Our data is now prepped in the variable "outs_stm" for future STM analysis.

```{r}
ads$txts=txts
temp_stm = textProcessor(documents=ads$txts,metadata=ads[,2:7])
outs_stm = prepDocuments(temp_stm$documents, temp_stm$vocab, temp_stm$meta)
```

## Determining *K* value

```{r}
# creating a tdm of documents
temp = Corpus(VectorSource(outs_stm$documents))
tdm = DocumentTermMatrix(temp)

# calculating lda tuning metrics - big picture overview
metrics <- FindTopicsNumber(
  tdm,
  topics = seq(from = 2, to = 100, by = 20),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics)

# calculating lda tuning metrics - 5-30

metrics_final <- FindTopicsNumber(
  tdm,
  topics = seq(from = 5, to = 30, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics_final)
```

```{r}
# think best is k=11
k <- 11
```

*Description and defense of method:* We want to minimize the Arun and CaoJuan metrics, and we want to maximize the Deveaud and Grifiths metrics. First, we calculated these metrics for a large K range (2-100) with large steps to observe the general pattern. It seems like the Arun, CaoJuan, and Deveaud metrics decrease over time, with the CaoJuan metric being the lowest around 22, the Griffiths metric increases over time. We then narrowed in on a K range from 5-30. From the plots, while the 4 metrics do not agree on a best K, it seems that 11 topics may be the most suitable when taking all metrics into account. While the Arun, CaoJuan, and Griffiths metrics are still moving, the Deveaud metric has a local peak. 

*Our choice of K:* 11

# Part 2

**Goal: Setting k = K, estimate the following topic models. For each model, produce a list of the top 10 most frequent works across each of the k topics**

## 2a: Unsupervised LDA model using lda::lda.collapsed.gibs.sampler(.., k)

Here we run the pre-processed string "txts" through the lexacalize fuction to enable use of the lda package. In this model we manually set k, alpha, and eta here for the remainder of lda() models. We define these model parameters as global variables that are then used for all lda models. We use the value of k determined above for the number of topics.

```{r}
#processing text for lda analysis 
vocab = docs[[2]]
result=list()

#preset parameters for lda() models
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)

#running the model!
result_2a=lda.collapsed.gibbs.sampler(docs[[1]], 		
	k,  			
	vocab,		
	100,  
	alpha = alpha, 
	eta = eta,   
	compute.log.likelihood=TRUE
)

```

Top Words
```{r}
lda_results <- top.topic.words(result_2a$topics,10)
colnames(lda_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(lda_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")

as.data.frame(lda_results)
```

## 2b: Supervised LDA model to predict *party* using lda::slda.em(..., party, k)

Next, we introduce party into our model. 

```{r}
# annotation to predict
y=ads$party

result_2b=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, 
		method="sLDA"
)
```

Top Words

```{r}
slda_party_results <- top.topic.words(result_2b$topics,10)

colnames(slda_party_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(slda_party_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(slda_party_results)
```

## 2c: Supervised LDA model to predict *tone* using lda::slda.em(..., tone, k)

```{r}
# annotation to predict
y=ads$tone

result_2c=slda.em(documents=docs[[1]],
		K=k,
		vocab=vocab,
		num.e.iterations=10,
		num.m.iterations=4,
		alpha=alpha, eta=eta,
		annotations=y,
		params,
		variance=0.25,
		lambda=1.0,
		logistic=FALSE, 
		method="sLDA"
)
```

Top Ten Words

```{r}
slda_tone_results <- top.topic.words(result_2c$topics,10)

colnames(slda_tone_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(slda_tone_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")

as.data.frame(slda_tone_results)
```

## 2d: Structural topic model using stm::stm(..., prevalence = ~ party + tone, k)

In work done in part 1, we cleaned our data using textProcessor() and prepDocuments() and determined the best value of k, so we are ready to run an sTM model.

```{r}
result_2d = stm(documents=outs_stm$documents,vocab=outs_stm$vocab,K=k,
	prevalence=~party+tone,data=outs_stm$meta)
```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2d)
```

For this assignment, we are interested in collecting the top 10 words in each category.

```{r}
stm_results_2d <- t(labelTopics(model=result_2d, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results_2d) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(stm_results_2d) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results_2d)
```

## 2e: Structural topic model using stm::stm(..., prevalence = ~ party + tone + office + policy + as.factor(state), k)

We repeat the same analysis as in 2d but use more variables for the modeling.

```{r}
result_2e = stm(documents=outs_stm$documents,vocab=outs_stm$vocab,K=k,
	prevalence=~party+tone+office+policy+as.factor(state),data=outs_stm$meta)
```

If we want to get a sense of only the 3 top words associated with each topic and also the prevalence of topics in order to sense-check our groups, we can use the plot function.

```{r}
# view 3 top words associated with each topic
plot(result_2e)
```

For this assignment, we are interested in collecting the top 10 words in each category.

```{r}
stm_results_2e <- t(labelTopics(model=result_2e, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)


colnames(stm_results_2e) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11")
rownames(stm_results_2e) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results_2e)
```

## Reflection

**Are these roughly the same topics, or are there major differences in the topics being estimated?**

We report the top ten words for each model specification and note both differences and similarities in the topics estimated.

Looking first at LDA models, there does seem to be variation in the topics produced by each model. The basic LDA model does seem to group topics in a logical fashion. For example, Topic #1 seems to reflect some sort of pro-families pro-business sentiment whereas topic #4 seems to focus on energy and the domestic economy. 

Turning to the party prediction models, topics follow what how we might a priori group words based on partisanship. Topic #1 seems to reflect a main stream GOP platform, focusing on taxation, immigration, and families. In contrast, Topic #6 focuses on grassroots democratic rhetoric, mentioning workers, families, business, and the economy. There are a few topics that seem poorly defined such as Topic #2 which seems to simply have filler words that were not eliminated with stop words, Topic #4 seems to capture words associated with an Alaskan senate race, and Topic #9 also seems to just capture erroneous rhetoric from the ads. 

Interestingly, the tone prediction models generate very different results. In some sense, the groupings seem to just reflect how one would group words based on their plain meaning. For example, Topic 1 groups the words "energi", which in context presumably refers to energy policy, with words about "chang". The tone here seems to be a positive one. Similarly, Topic 9 seems to just group a series of words related to money. However, it is not completely clear what organizing principle is captured by each of the topics or what content is embodied in "tone".

Now, looking at the STM models, the first model based on party and tone produces some of the same general topics we've seen in other models. We again see a energy-based topic: We again see a money-based topic: Topic 1 has words like "wall," "street," "dollar," and "million." We again see a work-based topic: Topic 4 has words like "work" and "job." We again see an immigration-based topic: Topic 5 has words like "immigra," "border," "mexico," and "secur."  Finally, we again see a war-based topic: Topic 7 has words like "veteran," "iraq" and "troop." Topic 10 has words like "energi," "oil," and "gas." We note that some topics feel less defined: Topic 9 includes aspects of both Democratic and Republican platforms with words like "liber" and "conserv."

Looking at the second STM model based on party, tone, office, policy, and state, we note very similar topics to the first STM model. We again note the associations between Topic 1 and money ("wall," "street," "dollar," "money"); Topic 4 and work ("work," "job"); Topic 5 and immigration ("illeg," "immigr," "border," "mexico," "secur"); Topic 7 and war ("secur," "veteran," "iraq," "troop"); and Topic 10 and energy ("energi," "oil," "gas"). We do note the appearance of some new emotional words like "fight" and "love," though it is unclear why the consideration of office, policy, and state introduces these new words

Though the models do differ, we note a degree of similarity in the inclusion of repeating major topics, such as energy, money, immigration, workers, and war, which do make sense when considering these are major areas of focus for politics. We are surprised that there is not more mentions about healthcare, another highly politicized area.