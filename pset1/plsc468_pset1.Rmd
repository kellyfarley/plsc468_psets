---
title: "PLSC 468 Pset 1"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "2/9/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
```

## PROBLEM 1: scraping Maine Encyclopedia

### 1a (in progress)

**Use RCurl::getURL to scrape 1968, 1972, 1976 state platforms for ME D and R parties**

```{r}
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
urls=c(urlsR,urlsD)

txts=getURL(urls)
```

**Use gsub and regular expressions to clean text**

```{r}
txts_raw=txts

# to restore
txts=txts_raw

# remove capitalization
txts=tolower(txts)

# remove punctuation and non-alpha-numerics
txts=gsub("\\W", " ", txts)

# remove numbers
txts=gsub("[0-9]", replace=" ", txts)

# remove particular characters
txts=gsub("[%&$*]", " ", txts)

# remove white space
txts=str_trim(txts)

# stemming (FIX)
#txts=stem_snowball(txts, algorithm="en")

# split at spaces
#txts=str_split(txts, " ")
#corpus=unlist(txts)

# remove stop words
#stopwords <- c("the", "or", "a", "at", "if", "be", "an")
#txts=txts[!txts %in% stopwords]

# remove most common words (TO DO)

# remove least common words (TO DO)

# remove empty entries
lapply(txts, function(z){ z[!is.na(z) & z != ""]})

txts_1a <- txts
```

**How well did you do?**

### 1b (done)

**Use htmlToText on raw platform text to redo cleaning**

```{r}
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))

source(paste(wd, "/functions/htmlToText.R", sep=""))

do.run=T
if(do.run==T){
  txts_1b=array(NA,length(urls))
  for(i in 1:length(urls)){
    txts_1b[i]=htmlToText(urls[i])
  }
}
```

**Comparison to 1a?  What are the remaining errant strings left here that your effort removed, if any? The reverse? Are the errant strings consistent across each of the platforms? Or is there much variation here?**

Compared to using gsub and regular expressions in 1a, it is much easier to use htmlToText in 1b - it is done with fewer lines of code and less manual effort to think about precisely what to clean.

Many of the html tags that were present in 1a have been removed: the paragraph tags, the div tags, the class tags, etc. However, punctuation, uppercase letters, and numbers were not removed, and the document was not stemmed, all issues that were addressed manually in 1a.

Errant strings do appear across both platforms: the end of line tags (n), the tab tags (t), etc. There is variation in precisely what is cleaned, though this can be adjusted if the user chooses to address more with gsub and regular expressions.

### 1c (done)

**Pick cleanest version and use tm package to turn text data into corpus object**

```{r}
corpus=Corpus(VectorSource(txts_1b))

corpus_raw=corpus
```

**Use tm_map and package functions to pre-process; transform to lowercase, remove punctuation, remove stop words**

```{r}
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)

corpus = tm_map(corpus, PlainTextDocument)

# remove uppercase
corpus = tm_map(corpus, tolower)

# remove stop words
corpus = tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus = tm_map(corpus, removePunctuation)

# remove extra white space
corpus = tm_map(corpus, stripWhitespace)
```

### 1d (to do)

**Term-document matrix with all remaining unigrams**

```{r}
dtm = DocumentTermMatrix(corpus)
```

**List of 20 most and 20 least frequent words across entire corpus** (to do)

```{r}
```

**List of 10 most frequent words for each of the 6 platforms**

```{r}
freq10 <- findMostFreqTerms(dtm, 10)
```

**Which words repeat among 3 R platforms? 3 D platforms? Across 1972 D and R platforms?**

```{r}

```

### 1e (to do)

**Most frequently repeating word from D and R platforms in 1982**

**2 association tables for each word using findAssocs: first D, then R**

**What 4 words have highest association with top 2 words for D v R? What might this tell us about policy differences between two parties?**

## PROBLEM 2: scraping presidential communications for the 117th House (in progress)

Goal: scrape abstract text, date, legislative committee(s) (if any)

Write a loop that will call series of urls for each message

Turn in: 4 matrix objects (one for each type of communication; Presidential Message, Executive Communication, Petition, Memorial), with 3 columns (url, abstract text, committee locations) and N rows

```{r eval=F}
#Goal: scrape abstract text, date, legislative committee(s) (if any)
```{r}

#loop to collect EXECUTIVE COMMUNICATION  
for (i in 3395:3390) 
{
  url=paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=3&r=",4000-i, sep='')
  print(url)
  obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p")
  text <- html_text(nodes)
  #collecting abstracts
  abstracts <- text[4]
  #dates (this is gross code sorry!)
  dates <- text[2]
}



#PRESIDENTIAL MESSAGE 
for(i in 19:1)
{
  pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
  print(pres_url)
  obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p")
  text <- html_text(nodes)
  #collecting abstracts - need to collect for each url 
  abstracts <- nodes[3]
  #dates
  dates <- texts[2]
  dates <- gsub(dates, pattern = "[a-z |A-Z ]", replace = "")
}





#PETITION


#MEMORIAL 

```

```{r eval = F}
# using different method: select gadget chrome extension
library(car)
library(leaps)
library(lubridate)
library(rvest)

url <- "https://www.congress.gov/search?q=%5B%7B%22source%22%3A%22house-communications%22%2C%22congress%22%3A%22117%22%7D%5D"
webpage <- read_html(url) 
webpage <- html_nodes(webpage, '.result-heading+ .result-item')
webpage <- html_text(webpage)
```
