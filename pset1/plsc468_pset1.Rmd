---
title: "PLSC 468 Pset 1"
author: "Kelly Farley"
date: "2/9/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# set working directory (change this for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
```

## PROBLEM 1: scraping Maine Encyclopedia

### 1a

**Use RCurl::getURL to scrape 1968, 1972, 1976 state platforms for ME D and R parties** (done)

```{r}
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
urls=c(urlsR,urlsD)

txts=getURL(urls)
```

**Use gsub and regular expressions to clean text** (in progress)

```{r}
txts_raw=txts

# to restore
txts=txts_raw

# remove capitalization
txts=tolower(txts)

# remove punctuation and non-alpha-numerics
txts=gsub("\\W", " ", txts)

# remove numbers
txts=gsub(txts, pattern="[0-9]", replace=" ")

# remove particular characters
txts=gsub("[%&$*]", " ", txts)

# remove white space
txts=str_trim(txts)

# stemming (FIX)
#txts=stem_snowball(txts, algorithm="en")

# split at spaces
#txts=str_split(txts, " ")
#corpus=unlist(txts)

# remove stop words
#stopwords <- c("the", "or", "a", "at", "if", "be", "an")
#txts=txts[!txts %in% stopwords]

# remove most common words (TO DO)

# remove least common words (TO DO)

# remove empty entries
lapply(txts, function(z){ z[!is.na(z) & z != ""]})

txts_1a <- txts

print("test")
```

**How well did you do?** (in progress)

### 1b

**Use htmlToText on raw platform text to redo cleaning** (done)

```{r}
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))

source(paste(wd, "functions/htmlToText.R", sep=""))

do.run=T
if(do.run==T){
  txts_1b=array(NA,length(urls))
  for(i in 1:length(urls)){
    txts_1b[i]=htmlToText(urls[i])
  }
}
```

**Comparison to 1a?  What are the remaining errant strings left here that your effort removed, if any? The reverse? Are the errant strings consistent across each of the platforms? Or is there much variation here?** (done)

### 1c

**Pick cleanest version and use tm package to turn text data into corpus object**

**Use tm_map and package functions to pre-process; transform to lowercase, remove punctuation, remove stop words**

### 1d

**Term-document matrix with all remaining unigrams**

**List of 20 most and 20 least frequent words across entire corpus**

**List of 10 most frequent words for each of the 6 platforms**

**Which words repeat among 3 R platforms? 3 D platforms? Across 1972 D and R platforms?**

### 1e

**Most frequently repeating word from D and R platforms in 1982**

**2 association tables for each word using findAssocs: first D, then R**

**What 4 words have highest association with top 2 words for D v R? What might this tell us about policy differences between two parties?**

## PROBLEM 2: scraping presidential communications for the 117th House

Goal: scrape abstract text, date, legislative committee(s) (if any)

Write a loop that will call series of urls for each message

Turn in: 4 matrix objects (one for each type of communication; Presidential Message, Executive Communication, Petition, Memorial), with 3 columns (url, abstract text, committee locations) and N rows
