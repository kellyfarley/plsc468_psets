---
title: "PLSC 468 Pset 1"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "2/9/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
```

## PROBLEM 1: scraping Maine Encyclopedia

### 1a (in progress)

**Use RCurl::getURL to scrape 1968, 1972, 1976 state platforms for ME D and R parties**

```{r}
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')

# combining urls
urls <- c(urlsD, urlsR)
```

**Use gsub and regular expressions to clean text**

```{r}
# looping thru the 6 party platforms
cleantxt <- NULL

for (i in 1:length(urls)){
  txts <- getURL(urls[i])
  
  # remove html attributes
  txts <- gsub("<[^<>]*>", "", txts)
  txts <- gsub("\t", "", txts, fixed = TRUE)
  txts <- gsub("\n", "", txts, fixed = TRUE)
  txts <- gsub("&#82[0-9]{2};", "", txts)
  
  # removing large amounts of white space
  txts <- gsub("\\s{2,}", "", txts)
  
  # removing text before preamble begins
  txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
  txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
  txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
  txts <-gsub(".*;+(?i)preamble", "", txts)
  
  # removing text after party platform ends
  txts <- gsub("Source: .*", "", txts)
  
  # removing extra characters
  txts <- gsub("â€\\S?", "", txts)
  txts <- gsub("*", "", txts, fixed = TRUE)
  
  # adding final cleaned text
  cleantxt[i] <- txts
}

cleantxt[6]

# issue - all caps words don't have spaces before and after, comes like that

cleantxt
```

**How well did you do?**

### 1b (done)

**Use htmlToText on raw platform text to redo cleaning**

```{r}
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))

source(paste(wd, "/functions/htmlToText.R", sep=""))

do.run=T
if(do.run==T){
  txts_1b=array(NA,length(urls))
  for(i in 1:length(urls)){
    txts_1b[i]=htmlToText(urls[i])
  }
}
```

**Comparison to 1a?  What are the remaining errant strings left here that your effort removed, if any? The reverse? Are the errant strings consistent across each of the platforms? Or is there much variation here?**

Compared to using gsub and regular expressions in 1a, it is much easier to use htmlToText in 1b - it is done with fewer lines of code and less manual effort to think about precisely what to clean.

Many of the html tags that were present in 1a have been removed: the paragraph tags, the div tags, the class tags, etc. However, punctuation, uppercase letters, and numbers were not removed, and the document was not stemmed, all issues that were addressed manually in 1a.

Errant strings do appear across both platforms: the end of line tags (n), the tab tags (t), etc. There is variation in precisely what is cleaned, though this can be adjusted if the user chooses to address more with gsub and regular expressions.

### 1c (done) - CHELSEA EDIT

**Pick cleanest version and use tm package to turn text data into corpus object**

```{r}
corpus=Corpus(VectorSource(cleantxt))
corpus_raw=corpus
```

**Use tm_map and package functions to pre-process; transform to lowercase, remove punctuation, remove stop words**

```{r}
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
#corpus = tm_map(corpus, PlainTextDocument)
# remove uppercase
corpus = tm_map(corpus, tolower)
# remove stop words
corpus = tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus = tm_map(corpus, removePunctuation)
# remove extra white space
corpus = tm_map(corpus, stripWhitespace)
# remove numbers
corpus = tm_map(corpus, removeNumbers)
```

### 1d (to do)

**Term-document matrix with all remaining unigrams**

```{r}
tdm = TermDocumentMatrix(corpus)
```

**List of 20 most and 20 least frequent words across entire corpus** (to do)

```{r}
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
```

**List of 10 most frequent words for each of the 6 platforms**

```{r}
freq10 <- findMostFreqTerms(tdm, 10)
freq10
```

**Which words repeat among 3 R platforms? 3 D platforms? Across 1972 D and R platforms?**

```{r}
# extracting the 10 most frequent words of each party platform
D1968 <- names(freq10[1][[1]])
D1972 <- names(freq10[2][[1]])
D1976 <- names(freq10[3][[1]])
R1968 <- names(freq10[4][[1]])
R1972 <- names(freq10[5][[1]])
R1976 <- names(freq10[6][[1]])

# words that repeaat among the 3 D platforms
Reduce(intersect, list(D1968,D1972,D1976))

# words that repeaat amonfg the 3 R platforms
Reduce(intersect, list(R1968,R1972,R1976))

# words that repeat across the 1972 D and R platforms
Reduce(intersect, list(D1972, R1972))
```

### 1e (to do)

**Most frequently repeating word from D and R platforms in 1982**

**2 association tables for each word using findAssocs: first D, then R**

**What 4 words have highest association with top 2 words for D v R? What might this tell us about policy differences between two parties?**

## PROBLEM 2: scraping presidential communications for the 117th House (in progress)

Goal: scrape abstract text, date, legislative committee(s) (if any)

Write a loop that will call series of urls for each message

Turn in: 4 matrix objects (one for each type of communication; Presidential Message, Executive Communication, Petition, Memorial), with 3 columns (url, abstract text, committee locations) and N rows

```{r eval = F}
#Goal: scrape abstract text, date, legislative committee(s) (if any)
exec <- data.frame(matrix(ncol=3,nrow=0))
names(exec) <-  c("url", "date", "abstract")

# will need to add 4th column for committee

for (i in 3414:3413) 
{
  url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
  print(url)
  obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p")
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  #dates (this is gross code sorry!)
  date <- text[2]
  date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
  date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
  thisDat <- c(url, date, abstract)
  exec <- rbind(exec, thisDat)
}
# need to clean up date and abstract t



#PRESIDENTIAL MESSAGE 
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
  pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
  print(pres_url)
  obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
   #collecting abstracts
  abstract <- text[3]
  date <- text[2]
  date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
  date <- gsub(date, pattern= "[:]", replacement = "")
  #NEED COMMITTEE
  thisDat <- c(url, date, abstract)
  pres <- rbind(pres, thisDat)
}

#cleaning dates code: 
#   dates <- gsub(dates, pattern = "[a-z |A-Z | :]", replace = "")


#PETITION
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
for(i in 96:94)
{
  pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
  print(pet_url)
  obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  date <- text[2]
  date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
  date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
  #NEED COMMITTEE
  thisDat <- c(url, date, abstract)
  pet <<- rbind(pet, thisDat)
}


#MEMORIAL 
mem <- data.frame(matrix(ncol=3,nrow=0))
names(mem) <-  c("url", "date", "abstract")
for(i in 138:137)
{
  mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
  print(mem_url)
  obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  date <- text[2]
  #NEED COMMITTEE
  thisDat <- c(url, date, abstract)
  mem <- rbind(mem, thisDat)
}


```

```{r eval = F}
# using different method: select gadget chrome extension
library(car)
library(leaps)
library(lubridate)
library(rvest)

url <- "https://www.congress.gov/search?q=%5B%7B%22source%22%3A%22house-communications%22%2C%22congress%22%3A%22117%22%7D%5D"
webpage <- read_html(url) 
webpage <- html_nodes(webpage, '.result-heading+ .result-item')
webpage <- html_text(webpage)
```
