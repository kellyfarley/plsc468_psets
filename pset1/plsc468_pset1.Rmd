---
title: "PLSC 468 Pset 1"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "2/18/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
```

## PROBLEM 1: scraping Maine Encyclopedia

### 1a

**Use RCurl::getURL to scrape 1968, 1972, 1976 state platforms for ME D and R parties**

```{r}
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years <- 1968+4*0:2
urlsR <- paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD <- paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')

# combining urls
urls <- c(urlsD, urlsR)

# print urls
paste(urls)
```

**Use gsub and regular expressions to clean text**

Note that only the text for the first url is printed to save space when reading the knitted HTML, but the text for all six urls is cleaned and stored in the variable cleantxt.

```{r}
# looping thru the 6 party platforms
cleantxt <- NULL

for (i in 1:length(urls)){
  txts <- getURL(urls[i])
  
  # remove html attributes
  txts <- gsub("<[^<>]*>", "", txts)
  txts <- gsub("\t", "", txts, fixed = TRUE)
  txts <- gsub("\n", "", txts, fixed = TRUE)
  txts <- gsub("&#82[0-9]{2};", "", txts)
  
  # removing large amounts of white space
  txts <- gsub("\\s{2,}", "", txts)
  
  # removing text before preamble begins
  txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
  txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
  txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
  txts <-gsub(".*;+(?i)preamble", "", txts)
  
  # removing text after party platform ends
  txts <- gsub("Source: .*", "", txts)
  
  # removing extra characters
  txts <- gsub("â€\\S?", "", txts)
  txts <- gsub("*", "", txts, fixed = TRUE)
  
  # adding final cleaned text
  cleantxt[i] <- txts
}

# print first entry to demonstrate cleaning
cleantxt[1]
```

**How well did you do?**

Using gsub and a series of regular expressions, we were able to remove all HTML attributes, text before and after the preamble + platform, and extraneous white spaces and extra characters. The capitalization and punctuation is preserved. We note that there is an issue with the all-caps words: there is not a space between between the all-caps word and its surrounding lowercase words, an issue we were unable to address. Overall, though, gsub proved to be an effective method for text cleaning.

### 1b

**Use htmlToText on raw platform text to redo cleaning**

Note that only the text for the first url is printed to save space when reading the knitted HTML, but the text for all six urls is cleaned and stored in the variable txts_1b.

```{r}
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))

# source htmlToText function
source(paste(wd, "/functions/htmlToText.R", sep=""))

do.run=T
if(do.run==T){
  txts_1b=array(NA,length(urls))
  for(i in 1:length(urls)){
    txts_1b[i]=htmlToText(urls[i])
  }
}

txts_1b[1]
```

**Comparison to 1a?  What are the remaining errant strings left here that your effort removed, if any? The reverse? Are the errant strings consistent across each of the platforms? Or is there much variation here?**

Compared to the use of gsub and regular expressions in 1a, it is much easier to use htmlToText in 1b - it is done with fewer lines of code and less manual effort to think about precisely what to clean. Much like in 1a, we extract the text with capitalization and punctuation preserved.

 Much of the work we did manually in 1a was done automatically in 1b, with htmlToText() removing many of the HTML attributes and extraneous white spaces.

However, here are errant strings present in 1b that we removed manually in 1a. There are many leftover HTML tags, such as \n for new lines and \t for tabs. There are leftover special characters, such as â€. There is also extraneous text from the website that is not related to the party platform.

We note that our issue with a lack of spacing around all-caps words in 1a is not present in 1b.

There are no errant strings that appear across both platforms since we addressed almost all of the errant strings manually in 1a. There is variation in precisely what is cleaned based on how thorough one is with gsub.

### 1c

**Pick cleanest version and use tm package to turn text data into corpus object**

We use the version from 1a, cleaned manually with gsub and regular expressions.

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(cleantxt))
corpus_raw <- corpus
```

**Use tm_map and package functions to pre-process; transform to lowercase, remove punctuation, remove stop words**

```{r}
# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)
```

### 1d

**Term-document matrix with all remaining unigrams**

```{r}
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
```

**List of 20 most and 20 least frequent words across entire corpus**

20 most common worsd across entire corpus:

```{r}
# combining the tdms of the 6 platforms into one matrix
m <- as.matrix(tdm)
# reorganizing the matrix to get the most common words
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
```

20 least common words across entire corpus:

```{r}
# reorganizing the maatrix to get the least common words
v2 <- sort(rowSums(m),decreasing=FALSE)
d2 <- data.frame(word = names(v2),freq=v2)
head(d2, 20)
```

**List of 10 most frequent words for each of the 6 platforms**

```{r}
freq10 <- findMostFreqTerms(tdm, 10)
freq10
```

**Which words repeat among 3 R platforms? 3 D platforms? Across 1972 D and R platforms?**

```{r}
# extracting the 10 most frequent words of each party platform
D1968 <- names(freq10[1][[1]])
D1972 <- names(freq10[2][[1]])
D1976 <- names(freq10[3][[1]])
R1968 <- names(freq10[4][[1]])
R1972 <- names(freq10[5][[1]])
R1976 <- names(freq10[6][[1]])

# words that repeat among the 3 R platforms
Reduce(intersect, list(R1968,R1972,R1976))

# words that repeat among the 3 D platforms
Reduce(intersect, list(D1968,D1972,D1976))

# words that repeat across the 1972 D and R platforms
Reduce(intersect, list(D1972, R1972))
```

Words repeating across the 3 Republican platforms: "state" and "main."

Words repeating across the 3 Democratic platforms: "state" and "public."

Words repeating across the 1972 Republican and Democratic platforms: "state" and "public."

### 1e (REVISE - potential error in most freq word in 1972, revise explanation)

**Most frequently repeating word from D and R platforms in 1972**

```{r}
# recall that D 1972 is the 2nd text in the corpus
freq10[2]

# recall that R 1972 is the 5th text in the corpus
freq10[5]
```

Most frequently repeating word from Democratic platform in 1972: main

Most frequently repeating word from Republican platform in 1972: support

**2 association tables for each word using findAssocs: first D, then R**


```{r}
# dem tdm
dtdm <- TermDocumentMatrix(corpus[1:3])

# republican tdm
rtdm <- TermDocumentMatrix(corpus[4:6])
```


```{r}
demFreq1972 <- "main"
repFreq1972 <- "support"

## dem platforms

#dem word - dem platforms
findAssocs(dtdm, demFreq1972, 0.9)

#repub word - dem platforms
findAssocs(dtdm, repFreq1972, 0.9)

## repub platforms

#dem word - repub platforms
findAssocs(rtdm, demFreq1972, 0.9)

#repub word - repub platforms
findAssocs(rtdm, repFreq1972, 0.9)
```

**What 4 words have highest association with top 2 words for D v R? What might this tell us about policy differences between two parties?**

ADD EXPLANATION!

## PROBLEM 2: scraping presidential communications for the 117th House

Goal: scrape abstract text, date, legislative committee(s) (if any)

Write a loop that will call series of urls for each message

Turn in: 4 matrix objects (one for each type of communication; Presidential Message, Executive Communication, Petition, Memorial), with 3 columns (url, abstract text, committee locations) and N rows

Note - to save space in the knitted HTML, we are only printing the head of each of the matrices, but the entirety is stored in the variables exec, pres, pet, and mem.

```{r}
# Goal: scrape url, abstract text,legislative committee(s) (if any)
# Process: initialize data frame; get urls; collect abstracts from "p" tag; collect committees from "li" tag; clean committee text; add to data frame

matNames <- c("URL", "Abstract", "Committee") # to be used for all matrices

# EXECUTIVE
exec <- data.frame(matrix(ncol=3,nrow=0))

for (i in 3414:3409) 
{
  url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
  obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
  # collecting abstracts
  nodes <- obs %>%
    html_nodes("p")
  text <- html_text(nodes)
  abstract <- text[3]
  # collecting committees
  comm_nodes <- obs %>%
    html_nodes("li") 
  # cleaning committees
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ") # to account for multiple committees
  # add to df
  thisDat <- c(url, abstract, committee)
  exec <- rbind(exec, thisDat)
  names(exec) <-  matNames
}

head(exec)

#PRESIDENTIAL MESSAGE 
pres <- data.frame(matrix(ncol=3,nrow=0))

for(i in 19:17)
{
  pres_url <- paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
  obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
  # collecting abstracts
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  abstract <- text[3]
  # collecting committees
  comm_nodes <- obs %>%
    html_nodes("li") 
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  # removes exactly in parens
  committee <- gsub("\n", "", committee, fixed = TRUE)
  # removes date
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  # add to df
  thisDat <- c(url,abstract, committee)
  pres <- rbind(pres, thisDat)
  names(pres) <-  matNames
}

head(pres)

#PETITION
pet <- data.frame(matrix(ncol=3,nrow=0))

for(i in 96:93)
{
  pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
  obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
  # collecting abstracts
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  abstract <- text[3]
  # collecting committees
  comm_nodes <- obs %>%
    html_nodes("li")
  # cleaning committees
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  # add to df
  thisDat <- c(url, abstract, committee)
  pet <- rbind(pet, thisDat)
  names(pet) <-  matNames
}

head(pet)

#MEMORIAL 
mem <- data.frame(matrix(ncol=3,nrow=0))

for(i in 138:135)
{
  mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
  obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
  # collecting abstracts
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  abstract <- text[3]
  # collecting committees
  comm_nodes <- obs %>%
    html_nodes("li") 
  # cleaning committees
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  # add to df
  thisDat <- c(url, abstract, committee)
  mem <- rbind(mem, thisDat)
  names(mem) <-  matNames
}

head(mem)
```