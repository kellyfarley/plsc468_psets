---
title: "PLSC 468 Pset 1"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "2/9/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
```

## PROBLEM 1: scraping Maine Encyclopedia

### 1a (add explanation)

**Use RCurl::getURL to scrape 1968, 1972, 1976 state platforms for ME D and R parties**

```{r}
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')

# combining urls
urls <- c(urlsD, urlsR)
```

**Use gsub and regular expressions to clean text**

```{r}
# looping thru the 6 party platforms
cleantxt <- NULL

for (i in 1:length(urls)){
  txts <- getURL(urls[i])
  
  # remove html attributes
  txts <- gsub("<[^<>]*>", "", txts)
  txts <- gsub("\t", "", txts, fixed = TRUE)
  txts <- gsub("\n", "", txts, fixed = TRUE)
  txts <- gsub("&#82[0-9]{2};", "", txts)
  
  # removing large amounts of white space
  txts <- gsub("\\s{2,}", "", txts)
  
  # removing text before preamble begins
  txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
  txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
  txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
  txts <-gsub(".*;+(?i)preamble", "", txts)
  
  # removing text after party platform ends
  txts <- gsub("Source: .*", "", txts)
  
  # removing extra characters
  txts <- gsub("â€\\S?", "", txts)
  txts <- gsub("*", "", txts, fixed = TRUE)
  
  # adding final cleaned text
  cleantxt[i] <- txts
}

cleantxt[6]

# issue - all caps words don't have spaces before and after, comes like that

cleantxt
```

**How well did you do?**

### 1b (revise explanation)

**Use htmlToText on raw platform text to redo cleaning**

```{r}
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))

source(paste(wd, "/functions/htmlToText.R", sep=""))

do.run=T
if(do.run==T){
  txts_1b=array(NA,length(urls))
  for(i in 1:length(urls)){
    txts_1b[i]=htmlToText(urls[i])
  }
}
```

**Comparison to 1a?  What are the remaining errant strings left here that your effort removed, if any? The reverse? Are the errant strings consistent across each of the platforms? Or is there much variation here?**

Compared to using gsub and regular expressions in 1a, it is much easier to use htmlToText in 1b - it is done with fewer lines of code and less manual effort to think about precisely what to clean.

Compared to 1a, However, punctuation, uppercase letters, and numbers were not removed, and the document was not stemmed, all issues that were addressed manually in 1a.

Errant strings do not appear across both platforms. There is variation in precisely what is cleaned, though this can be adjusted if the user chooses to address more with gsub and regular expressions.

### 1c (done)

**Pick cleanest version and use tm package to turn text data into corpus object**

We use the version from 1a.

```{r}
corpus=Corpus(VectorSource(cleantxt))
corpus_raw=corpus
```

**Use tm_map and package functions to pre-process; transform to lowercase, remove punctuation, remove stop words**

```{r}
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
#corpus = tm_map(corpus, PlainTextDocument)
# remove uppercase
corpus = tm_map(corpus, tolower)
# remove stop words
corpus = tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus = tm_map(corpus, removePunctuation)
# remove extra white space
corpus = tm_map(corpus, stripWhitespace)
# remove numbers
corpus = tm_map(corpus, removeNumbers)
```

### 1d (add least frequent)

**Term-document matrix with all remaining unigrams**

```{r}
tdm = TermDocumentMatrix(corpus)
```

**List of 20 most and 20 least frequent words across entire corpus** (add least frequent)

```{r}
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
```

**List of 10 most frequent words for each of the 6 platforms**

```{r}
freq10 <- findMostFreqTerms(tdm, 10)
freq10
```

**Which words repeat among 3 R platforms? 3 D platforms? Across 1972 D and R platforms?**

```{r}
# extracting the 10 most frequent words of each party platform
D1968 <- names(freq10[1][[1]])
D1972 <- names(freq10[2][[1]])
D1976 <- names(freq10[3][[1]])
R1968 <- names(freq10[4][[1]])
R1972 <- names(freq10[5][[1]])
R1976 <- names(freq10[6][[1]])

# words that repeaat amonfg the 3 R platforms
Reduce(intersect, list(R1968,R1972,R1976))

# words that repeaat among the 3 D platforms
Reduce(intersect, list(D1968,D1972,D1976))

# words that repeat across the 1972 D and R platforms
Reduce(intersect, list(D1972, R1972))
```

Words repeating across the 3 Republican platforms: "state" and "main."

Words repeating across the 3 Democratic platforms: "state" and "public."

Words repeating across the 1972 Republican and Democratic platforms: "state" and "public."

### 1e (to do)

**Most frequently repeating word from D and R platforms in 1982**

**2 association tables for each word using findAssocs: first D, then R**

**What 4 words have highest association with top 2 words for D v R? What might this tell us about policy differences between two parties?**

## PROBLEM 2: scraping presidential communications for the 117th House (in progress)

Goal: scrape abstract text, date, legislative committee(s) (if any)

Write a loop that will call series of urls for each message

Turn in: 4 matrix objects (one for each type of communication; Presidential Message, Executive Communication, Petition, Memorial), with 3 columns (url, abstract text, committee locations) and N rows

```{r eval = F}
#Goal: scrape abstract text, date, legislative committee(s) (if any)
exec <- data.frame(matrix(ncol=3,nrow=0))

# will need to add 4th column for committee
for (i in 3414:1) 
{
  url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
  obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p")
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  #committees
    comm_nodes <- obs %>%
  html_nodes("li") 
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  #datafram
  thisDat <- c(url, abstract,committee)
  exec <- rbind(exec, thisDat)
  names(exec) <-  c("URL", "Abstract", "Committee")
}



#PRESIDENTIAL MESSAGE 
pres <- data.frame(matrix(ncol=3,nrow=0))
for(i in 19:1)
{
  pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
  obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  #COMMITTEE
  comm_nodes <- obs %>%
  html_nodes("li") 
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  #removes exactly in parens
  committee <- gsub("\n", "", committee, fixed = TRUE)
  #removing date
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  #making df 
  thisDat <- c(url,abstract, committee)
  pres <- rbind(pres, thisDat)
  names(pres) <-  c("URL", "Abstract","Committee")
}



#PETITION
pet <- data.frame(matrix(ncol=3,nrow=0))
for(i in 96:1)
{
  pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
  obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  #committees
    comm_nodes <- obs %>%
  html_nodes("li") 
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  #dataframe
  thisDat <- c(url, abstract, committee)
  pet <<- rbind(pet, thisDat)
  names(pet) <-  c("URL", "Abstract","Committee")
}


#MEMORIAL 
mem <- data.frame(matrix(ncol=3,nrow=0))
for(i in 138:1)
{
  mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
  obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
  nodes <- obs %>%
    html_nodes("p") 
  text <- html_text(nodes)
  #collecting abstracts
  abstract <- text[3]
  #committees
  comm_nodes <- obs %>%
  html_nodes("li") 
  x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
  committee <- comm_nodes[x]
  committee <- gsub("<[^<>]*>", "", committee)
  committee <- gsub("\n", "", committee, fixed = TRUE)
  committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
  committee <- gsub(" — ", "", committee, fixed = TRUE)
  committee <- paste(committee, collapse=", ")
  thisDat <- c(url, abstract, committee)
  mem <- rbind(mem, thisDat)
  names(mem) <-  c("URL", "Abstract", "Committee")
}


```