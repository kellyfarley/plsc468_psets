#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
str(thisDat)
str(pres)
rbind(pres, thisDat)
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- as.data.frame(c(url, date, abstract))
str(thisDat)
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- data.frame(c(url, date, abstract))
thisDat <- data.frame(c(url, date, abstract))
pres <- rbind(pres, thisDat)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
#Goal: scrape abstract text, date, legislative committee(s) (if any)
exec <- data.frame(matrix(ncol=3,nrow=0))
names(exec) <-  c("url", "date", "abstract")
# will need to add 4th column for committee
#loop to collect EXECUTIVE COMMUNICATION
for (i in 3395:3390)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=3&r=",4000-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[4]
#dates (this is gross code sorry!)
date <- text[2]
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
# need to clean up date and abstract t
#PRESIDENTIAL MESSAGE
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- data.frame(c(url, date, abstract))
pres <- rbind(pres, thisDat)
}
#cleaning dates code:
#   dates <- gsub(dates, pattern = "[a-z |A-Z | :]", replace = "")
#PETITION
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
for(i in 96:95)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
#MEMORIAL
mem <- data.frame(matrix(ncol=3,nrow=0))
names(mem) <-  c("url", "date", "abstract")
for(i in 138:137)
{
mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
print(mem_url)
obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
mem <- rbind(mem, thisDat)
}
rm(list = ls()) # clear global environ
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
exec <- data.frame(matrix(ncol=3,nrow=0))
tibble(exec)
names(exec) <-  c("url", "date", "abstract")
# will need to add 4th column for committee
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
exec
View(exec)
3414:3413
View(exec)
exec[1]
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
# chelsea test 11
exec <- data.frame(matrix(ncol=3,nrow=0))
#tibble(exec)
#names(exec) <-  c("url", "date", "abstract")
# will need to add 4th column for committee
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
names(exec) <- c("url", "date", "abstract")
}
View(exec)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
# combining urls
urls <- c(urlsD, urlsR)
# looping thru the 6 party platforms
cleantxt <- NULL
for (i in 1:length(urls)){
txts <- getURL(urls[i])
# remove html attributes
txts <- gsub("<[^<>]*>", "", txts)
txts <- gsub("\t", "", txts, fixed = TRUE)
txts <- gsub("\n", "", txts, fixed = TRUE)
txts <- gsub("&#82[0-9]{2};", "", txts)
# removing large amounts of white space
txts <- gsub("\\s{2,}", "", txts)
# removing text before preamble begins
txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
txts <-gsub(".*;+(?i)preamble", "", txts)
# removing text after party platform ends
txts <- gsub("Source: .*", "", txts)
# removing extra characters
txts <- gsub("â€\\S?", "", txts)
txts <- gsub("*", "", txts, fixed = TRUE)
# adding final cleaned text
cleantxt[i] <- txts
}
cleantxt[6]
# issue - all caps words don't have spaces before and after, comes like that
cleantxt
corpus=Corpus(VectorSource(cleantxt))
corpus_raw=corpus
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
#corpus = tm_map(corpus, PlainTextDocument)
# remove uppercase
corpus = tm_map(corpus, tolower)
# remove stop words
corpus = tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus = tm_map(corpus, removePunctuation)
# remove extra white space
corpus = tm_map(corpus, stripWhitespace)
# remove numbers
corpus = tm_map(corpus, removeNumbers)
corpus
tdm = TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
freq10 <- findMostFreqTerms(tdm, 10)
freq10
# extracting the 10 most frequent words of each party platform
D1968 <- names(freq10[1][[1]])
D1972 <- names(freq10[2][[1]])
D1976 <- names(freq10[3][[1]])
R1968 <- names(freq10[4][[1]])
R1972 <- names(freq10[5][[1]])
R1976 <- names(freq10[6][[1]])
# words that repeaat among the 3 D platforms
Reduce(intersect, list(D1968,D1972,D1976))
# words that repeaat amonfg the 3 R platforms
Reduce(intersect, list(R1968,R1972,R1976))
# words that repeat across the 1972 D and R platforms
Reduce(intersect, list(D1972, R1972))
exec <- data.frame(matrix(ncol=3,nrow=0))
tibble(exec)
exec <- data.frame(matrix(ncol=3,nrow=0))
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
names(exec) <-  c("url", "date", "abstract")
}
exec
View(exec)
View(exec)
i<-3414
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
names(exec) <-  c("url", "date", "abstract")
text
nodes <- obs %>%
html_nodes("li")
nodes
nodes[15]
nodes[16]
View(nodes)
nodes[17]
View(nodes[17])
nodes[[17]]
nodes
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
pres
View(pres)
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
for(i in 96:94)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
pet
View(pet)
mem <- data.frame(matrix(ncol=3,nrow=0))
names(mem) <-  c("url", "date", "abstract")
for(i in 138:137)
{
mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
print(mem_url)
obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
mem <- rbind(mem, thisDat)
}
View(mem)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
# combining urls
urls <- c(urlsD, urlsR)
# looping thru the 6 party platforms
cleantxt <- NULL
for (i in 1:length(urls)){
txts <- getURL(urls[i])
# remove html attributes
txts <- gsub("<[^<>]*>", "", txts)
txts <- gsub("\t", "", txts, fixed = TRUE)
txts <- gsub("\n", "", txts, fixed = TRUE)
txts <- gsub("&#82[0-9]{2};", "", txts)
# removing large amounts of white space
txts <- gsub("\\s{2,}", "", txts)
# removing text before preamble begins
txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
txts <-gsub(".*;+(?i)preamble", "", txts)
# removing text after party platform ends
txts <- gsub("Source: .*", "", txts)
# removing extra characters
txts <- gsub("â€\\S?", "", txts)
txts <- gsub("*", "", txts, fixed = TRUE)
# adding final cleaned text
cleantxt[i] <- txts
}
cleantxt[6]
# issue - all caps words don't have spaces before and after, comes like that
cleantxt
cleantxt
mem <- data.frame(matrix(ncol=3,nrow=0))
for(i in 138:137)
{
mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
print(mem_url)
obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#committees
comm_nodes <- obs %>%
html_nodes("li")
x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
committee <- comm_nodes[x]
committee <- gsub("<[^<>]*>", "", committee)
committee <- gsub("\n", "", committee, fixed = TRUE)
committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
committee <- gsub(" — ", "", committee, fixed = TRUE)
committee <- paste(committee, collapse="")
thisDat <- c(url, committee, abstract)
mem <- rbind(mem, thisDat)
names(mem) <-  c("url", "committee", "abstract")
}
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
mem <- data.frame(matrix(ncol=3,nrow=0))
for(i in 138:137)
{
mem_url=paste("https://www.congress.gov/house-communication/117th-congress/memorial/",i,"?s=3&r=", 139-i, sep='')
print(mem_url)
obs <- read_html(GET(mem_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#committees
comm_nodes <- obs %>%
html_nodes("li")
x <- grep("[0-9]{2}/[0-9]{2}/[0-9]{4}", comm_nodes)
committee <- comm_nodes[x]
committee <- gsub("<[^<>]*>", "", committee)
committee <- gsub("\n", "", committee, fixed = TRUE)
committee <- gsub("[0-9]{2}/[0-9]{2}/[0-9]{4}", "", committee)
committee <- gsub(" — ", "", committee, fixed = TRUE)
committee <- paste(committee, collapse="")
thisDat <- c(url, committee, abstract)
mem <- rbind(mem, thisDat)
names(mem) <-  c("url", "committee", "abstract")
}
