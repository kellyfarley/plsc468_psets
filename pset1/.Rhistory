exec
date <- text[2]
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "Committee on [a-z|A-Z]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
date
gsub(date, pattern= "Committee on [a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "Committee on [a-z|A-Z]", replacement = "")
date
date
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
date
date <- gsub(date, pattern= "[:]", replacement = "")
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
date
pres
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
pres[2]
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
for(i in 96:94)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
rm(list = ls()) # clear global environ
rm(list = ls()) # clear global environ
rm(list = ls()) # clear global environ
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(httr)
library(rvest)
exec <- data.frame(matrix(ncol=3,nrow=0))
tibble(exec)
names(exec) <-  c("url", "date", "abstract")
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
date
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
#date <- gsub(date, pattern= "[:]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
date
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
date
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[ˆ\/]", replacement = "")
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[ˆ\ /]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
date
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "ˆ[\ /]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
date
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[^ /]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
# ne
date
# will need to add 4th column for committee
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
pet <- data.frame(matrix(ncol=3,nrow=0))
names(pet) <-  c("url", "date", "abstract")
for(i in 96:94)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
date
for(i in 96:94)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
date
for(i in 96:94)
{
pet_url=paste("https://www.congress.gov/house-communication/117th-congress/petition/",i,"?s=3&r=", 97-i, sep='')
print(pet_url)
obs <- read_html(GET(pet_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pet <<- rbind(pet, thisDat)
}
pet[3]
pet[2]
View(pet)
install.packages("corpus")
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
install.packages("SnowballC")
library(SnowballC)
library(corpus)
# extracting the 10 most frequent words of each party platform
D1968 <- names(freq10[1][[1]])
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
library(corpus)
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
install.packages("corpus")
install.packages("corpus")
library(corpus)
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
library(tm)
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
install.packages(textreadr)
install.packages("textreadr")
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
# combining urls
urls <- c(urlsD, urlsR)
# looping thru the 6 party platforms
cleantxt <- NULL
for (i in 1:length(urls)){
txts <- getURL(urls[i])
# remove html attributes
txts <- gsub("<[^<>]*>", "", txts)
txts <- gsub("\t", "", txts, fixed = TRUE)
txts <- gsub("\n", "", txts, fixed = TRUE)
txts <- gsub("&#82[0-9]{2};", "", txts)
# removing large amounts of white space
txts <- gsub("\\s{2,}", "", txts)
# removing text before preamble begins
txts <- gsub(".*description", "", txts) # "description" tag always comes right before content
txts <- gsub(".*\\(\\);\\}\\);", "", txts) # another tag before content
txts <- gsub(".*(?i)[aA-zZ]preamble", "", txts)
txts <-gsub(".*;+(?i)preamble", "", txts)
# removing text after party platform ends
txts <- gsub("Source: .*", "", txts)
# removing extra characters
txts <- gsub("â€\\S?", "", txts)
txts <- gsub("*", "", txts, fixed = TRUE)
# adding final cleaned text
cleantxt[i] <- txts
}
cleantxt[6]
# issue - all caps words don't have spaces before and after, comes like that
cleantxt
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
exec <- data.frame(matrix(ncol=3,nrow=0))
tibble(exec)
names(exec) <-  c("url", "date", "abstract")
# will need to add 4th column for committee
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
rm(list = ls()) # clear global environ
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
library(textreadr)
library(httr)
library(rvest)
#Goal: scrape abstract text, date, legislative committee(s) (if any)
exec <- data.frame(matrix(ncol=3,nrow=0))
tibble(exec)
names(exec) <-  c("url", "date", "abstract")
# will need to add 4th column for committee
for (i in 3414:3413)
{
url <- paste("https://www.congress.gov/house-communication/117th-congress/executive-communication/",i,"?s=1&r=",3415-i, sep='')
print(url)
obs <- read_html(GET(url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
#dates (this is gross code sorry!)
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:|,|.|-]", replacement = "")
thisDat <- c(url, date, abstract)
exec <- rbind(exec, thisDat)
}
pres <- data.frame(matrix(ncol=3,nrow=0))
names(pres) <-  c("url", "date", "abstract")
for(i in 19:11)
{
pres_url=paste("https://www.congress.gov/house-communication/117th-congress/presidential-message/",i,"?s=3&r=", 20-i, sep='')
print(pres_url)
obs <- read_html(GET(pres_url, config(ssl_verifypeer = FALSE)))
nodes <- obs %>%
html_nodes("p")
text <- html_text(nodes)
#collecting abstracts
abstract <- text[3]
date <- text[2]
date <- gsub(date, pattern= "[a-z|A-Z]", replacement = "")
date <- gsub(date, pattern= "[:]", replacement = "")
#NEED COMMITTEE
thisDat <- c(url, date, abstract)
pres <- rbind(pres, thisDat)
}
