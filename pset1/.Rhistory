rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change this for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
url ="https://www.congress.gov/search?q=%5B%7B%22source%22%3A%22house-communications%22%2C%22congress%22%3A%22117%22%7D%5D"
library(car)
library(car)
library(leaps)
library(lubridate)
library(rvest)
url <- "https://www.congress.gov/search?q=%5B%7B%22source%22%3A%22house-communications%22%2C%22congress%22%3A%22117%22%7D%5D"
webpage <- read_html(url)
webpage <- read_html(url)
companyRank <- html_nodes(webpage, '.result-heading+ .result-item')
companyRank
companyRank[1]
companyRank[[1]]
str(companyRank)
companyRank[1][1]
companyRank[1][[1]]
View(companyRank)
companyRank
temp <- html_text(companyRank)
temp
library(car)
library(leaps)
library(lubridate)
library(rvest)
url <- "https://www.congress.gov/search?q=%5B%7B%22source%22%3A%22house-communications%22%2C%22congress%22%3A%22117%22%7D%5D"
webpage <- read_html(url)
webpage <- html_nodes(webpage, '.result-heading+ .result-item')
webpage <- html_text(webpage)
View(webpage)
str(webpage)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(RCurl)
library(stringr)
library(tm)
library(pryr)
library(corpus)
library(SnowballC)
# get urls for both republican and democrat platforms for 1968, 1972, 1976
years=1968+4*0:2
urlsR=paste("https://maineanencyclopedia.com/republican-party-platform-",years,"/",sep='')
urlsD=paste("https://maineanencyclopedia.com/democratic-party-platform-",years,"/",sep='')
urls=c(urlsR,urlsD)
txts=getURL(urls)
txts_raw=txts
# to restore
txts=txts_raw
# remove capitalization
txts=tolower(txts)
# remove punctuation and non-alpha-numerics
txts=gsub("\\W", " ", txts)
# remove numbers
txts=gsub("[0-9]", replace=" ", txts)
# remove particular characters
txts=gsub("[%&$*]", " ", txts)
# remove white space
txts=str_trim(txts)
# stemming (FIX)
#txts=stem_snowball(txts, algorithm="en")
# split at spaces
#txts=str_split(txts, " ")
#corpus=unlist(txts)
# remove stop words
#stopwords <- c("the", "or", "a", "at", "if", "be", "an")
#txts=txts[!txts %in% stopwords]
# remove most common words (TO DO)
# remove least common words (TO DO)
# remove empty entries
lapply(txts, function(z){ z[!is.na(z) & z != ""]})
txts_1a <- txts
# download htmlToText function
download.file("https://drive.google.com/u/0/uc?id=1LuMKRhzPBHWGzVM-Bs64n_ciNMoHHbPd", paste(wd, "/functions/htmlToText.R", sep=""))
source(paste(wd, "/functions/htmlToText.R", sep=""))
do.run=T
if(do.run==T){
txts_1b=array(NA,length(urls))
for(i in 1:length(urls)){
txts_1b[i]=htmlToText(urls[i])
}
}
corpus=Corpus(VectorSource(txts_1b))
corpus_raw=corpus
# stem using porter algorithm
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, PlainTextDocument)
# remove uppercase
corpus = tm_map(corpus, tolower)
# remove stop words
corpus = tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus = tm_map(corpus, removePunctuation)
# remove extra white space
corpus = tm_map(corpus, stripWhitespace)
dtm = DocumentTermMatrix(corpus)
freq10 <- findMostFreqTerms(dtm, 10)
