annotate(geom = "text", x = 2007, y = 1, label = "Median", color = "red", size = 3) +
annotate(geom = "text", x = 2007, y = -0.4, label = "Median", color = "blue", size = 3) +
theme(
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
text = element_text(family = "Palatino", size = 10),
axis.title.y = element_text(face = "bold"),
axis.title.x = element_text(face = "bold"),
)
dat <- inner_join(dat, sumMQ[,c(1:2)], by="term")
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))
# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)
# remove uppercase
corpus <- tm_map(corpus, tolower)
# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
# reorganizing the matrix to get the most common words
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)
# note: consider removing some of these most common words
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(2, "Paired"))
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
mostfreqwords <- d[1:10, 1]
dec60 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1970 & dat$term >= 1960)]))
v_60 <- sort(rowSums(dec60),decreasing=TRUE)
d_60 <- data.frame(word = names(v_60),freq=v_60)
dec70 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1980 & dat$term >= 1970)]))
v_70 <- sort(rowSums(dec70),decreasing=TRUE)
d_70 <- data.frame(word = names(v_70),freq=v_70)
dec80 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1990 & dat$term >= 1980)]))
v_80 <- sort(rowSums(dec80),decreasing=TRUE)
d_80 <- data.frame(word = names(v_80),freq=v_80)
dec90 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2000 & dat$term >= 1990)]))
v_90 <- sort(rowSums(dec90),decreasing=TRUE)
d_90 <- data.frame(word = names(v_90),freq=v_90)
dec00 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2010 & dat$term >= 2000)]))
v_00 <- sort(rowSums(dec00),decreasing=TRUE)
d_00 <- data.frame(word = names(v_00),freq=v_00)
dec10 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2020 & dat$term >= 2010)]))
v_10 <- sort(rowSums(dec10),decreasing=TRUE)
d_10 <- data.frame(word = names(v_10),freq=v_10)
# merge all data frames together
decade_list <- list(d_70, d_80, d_90, d_00, d_10)
decades <- decade_list %>% reduce(full_join, by="word")
names(decades) <- c("word", "1970s", "1980s", "1990s", "2000s", "2010s")
# calculate relative frequencies by dividing by number of cases
decades_relative <- decades
decades_relative$`1970s` <- decades$`1970s` / sum(na.omit(decades$`1970s`))
decades_relative$`1980s` <- decades$`1980s` / sum(na.omit(decades$`1980s`))
decades_relative$`1990s` <- decades$`1990s` / sum(na.omit(decades$`1990s`))
decades_relative$`2000s` <- decades$`2000s` / sum(na.omit(decades$`2000s`))
decades_relative$`2010s` <- decades$`2010s` / sum(na.omit(decades$`2010s`))
# most frequent
decades_relative_top <- decades_relative %>%
filter(word %in% mostfreqwords) %>%
pivot_longer(cols = c(2:6)) %>%
mutate(name = as.numeric(substr(name, 1, 4))) %>%
rename(year = name,
proportion = value)
ggplot(decades_relative_top, aes(x=year, y=proportion, group=word, color = word)) +
geom_line() +
geom_point() +
theme_minimal() +
xlab("Term") +
scale_x_continuous(breaks=seq(1970, 2020, 10)) +
ylab("Frequency of Word") +
theme(
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
text = element_text(family = "Palatino", size = 10),
axis.title.y = element_text(face = "bold"),
axis.title.x = element_text(face = "bold"),
)
civilRights <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Civil Rights")]))
v_cr <- sort(rowSums(civilRights),decreasing=TRUE)
d_cr <- data.frame(word = names(v_cr),freq=v_cr)
barplot(d_cr[1:10,]$freq, las = 2, names.arg = d_cr[1:10,]$word,
col ="lightblue", main ="Most Frequent Words: Civil Rights",
ylab = "Word frequencies")
criminalProcedure <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Criminal Procedure")]))
v_cp <- sort(rowSums(criminalProcedure),decreasing=TRUE)
d_cp <- data.frame(word = names(v_cp),freq=v_cp)
barplot(d_cp[1:10,]$freq, las = 2, names.arg = d_cp[1:10,]$word,
col ="lightblue", main ="Most Frequent Words: Criminal Procedure",
ylab = "Word frequencies")
economicActivity <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Economic Activity")]))
v_ea <- sort(rowSums(economicActivity),decreasing=TRUE)
d_ea <- data.frame(word = names(v_ea),freq=v_ea)
barplot(d_ea[1:10,]$freq, las = 2, names.arg = d_ea[1:10,]$word,
col ="lightblue", main ="Most Frequent Words: Economic Activity",
ylab = "Word frequencies")
library(lda)
library(topicmodels)
library(stm)
library(stm)
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))
# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)
# remove uppercase
corpus <- tm_map(corpus, tolower)
# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
View(corpus)
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
# wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
# load libraries
library(tidyverse)
library(stringr)
library(tm)
library(lda)
library(topicmodels)
library(stm)
library(ldatuning)
library(SnowballC)
set.seed(1005)
load(file = "pset3/adData.Rdata")
load(file = "adData.Rdata")
txts=dat$facts
# court data source: https://github.com/smitp415/CSCI_544_Final_Project
raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) #
dat <- raw
# justice data source: https://mqscores.lsa.umich.edu/measures.php
justices <- read.csv("final_project/justices.csv")
raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) #
dat <- raw
dat
# justice data source: https://mqscores.lsa.umich.edu/measures.php
justices <- read.csv("justices.csv")
dim(dat)
dat <- dat %>%
select(name, href, term, facts, majority_vote, minority_vote, decision_type, disposition, issue_area) %>%
mutate(term = case_when(term == "1789-1850" ~ round(mean(c(1789, 1850)), 0),
term == "1850-1900" ~ round(mean(c(1850, 1900)), 0),
term == "1900-1940" ~ round(mean(c(1900, 1940)), 0),
term == "1940-1955" ~ round(mean(c(1940, 1955)), 0),
TRUE ~ as.numeric(term))) %>%
mutate(term = as.numeric(term),
decision_type = as.factor(decision_type),
disposition = as.factor(disposition),
issue_area = as.factor(issue_area))
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
txts=dat$facts
load(file = "adData.Rdata")
temp_stm = textProcessor(documents=dat$txts,metadata=dat[,2:9])
outs_stm = prepDocuments(temp_stm$documents, temp_stm$vocab, temp_stm$meta)
txts=dat$facts
temp_stm = textProcessor(documents=dat$txts,metadata=dat[,2:9])
outs_stm = prepDocuments(temp_stm$documents, temp_stm$vocab, temp_stm$meta)
txts=dat$facts
temp_stm = textProcessor(documents=dat$txts,metadata=dat[,2:7])
#library(topicmodels) # has model for correlated topics w/o covariates
load('platforms.Rdata')
outs_stm = prepDocuments(temp_stm$documents, temp_stm$vocab, temp_stm$meta)
txts=dat$facts
temp_stm = textProcessor(documents=dat$txts,metadata=dat[,1:9])
View(temp_stm)
txts=dat$facts
docs=lexicalize(txts)
docs=lexicalize(txts)
txts
View(docs)
View(docs)
#processing text for lda analysis
vocab = docs[[2]]
txts=dat$facts
docs=lexicalize(txts)
#processing text for lda analysis
vocab = docs[[2]]
result=list()
#preset parameters for lda() models
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)
#preset parameters for lda() models
k=5
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)
test=lda.collapsed.gibbs.sampler(docs[[1]],
k,
vocab,
100,
alpha = alpha,
eta = eta,
compute.log.likelihood=TRUE
)
lda_results <- top.topic.words(test$topics,10)
lda_results
colnames(lda_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7",
"Topic 8", "Topic 9", "Topic 10", "Topic 11")
colnames(lda_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", "Topic 8", "Topic 9", "Topic 10", "Topic 11")
lda_results
colnames(lda_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", "Topic 8", "Topic 9", "Topic 10")
lda_results <- top.topic.words(test$topics,10)
as.data.frame(lda_results)
# annotation to predict
y=dat$issue_area
result_isse=slda.em(documents=docs[[1]],
K=k,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha, eta=eta,
annotations=y,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
result_isse=slda.em(documents=docs[[1]],
K=5,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha, eta=eta,
annotations=y,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
# annotation to predict
y=ads$party
result_2b=slda.em(documents=docs[[1]],
K=5,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha,
eta=eta,
annotations=ads$party,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
result_issue=slda.em(documents=docs[[1]],
K=5,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha,
eta=eta,
annotations=dat$issue_area,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))
# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)
# remove uppercase
corpus <- tm_map(corpus, tolower)
# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
txts=dat$facts
txts=dat$facts
docs=lexicalize(txts)
#processing text for lda analysis
vocab = docs[[2]]
result=list()
#preset parameters for lda() models
k=5
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)
#running the model!
test=lda.collapsed.gibbs.sampler(docs[[1]],
k,
vocab,
100,
alpha = alpha,
eta = eta,
compute.log.likelihood=TRUE
)
lda_results <- top.topic.words(test$topics,10)
lda_results
View(test)
rm(list = ls()) # clear global environ
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
# set working directory (change for each user)
wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
# set working directory (change for each user)
wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
# set working directory (change for each user)
wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(lda)
library(topicmodels)
library(stm)
set.seed(1005)
# court data source: https://github.com/smitp415/CSCI_544_Final_Project
raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) #
dat <- raw
# justice data source: https://mqscores.lsa.umich.edu/measures.php
justices <- read.csv("justices.csv")
dat <- dat %>%
select(name, href, term, facts, majority_vote, minority_vote, decision_type, disposition, issue_area) %>%
mutate(term = case_when(term == "1789-1850" ~ round(mean(c(1789, 1850)), 0),
term == "1850-1900" ~ round(mean(c(1850, 1900)), 0),
term == "1900-1940" ~ round(mean(c(1900, 1940)), 0),
term == "1940-1955" ~ round(mean(c(1940, 1955)), 0),
TRUE ~ as.numeric(term))) %>%
mutate(term = as.numeric(term),
decision_type = as.factor(decision_type),
disposition = as.factor(disposition),
issue_area = as.factor(issue_area))
dat <- dat %>%
na.omit()
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))
# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)
# remove uppercase
corpus <- tm_map(corpus, tolower)
# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
txts=dat$facts
docs=lexicalize(txts)
temp = textProcessor(documents=dat$facts,metadata=dat[,1:9])
outs_stm= prepDocuments(temp$documents, temp$vocab, temp$meta)
k <- 12
#processing text for lda analysis
vocab = docs[[2]]
result=list()
#preset parameters for lda() models
alpha = 1.0
eta = 0.1
params = sample(c(-1, 1), k, replace=TRUE)
#running the model!
test=lda.collapsed.gibbs.sampler(docs[[1]],
k,
vocab,
100,
alpha = alpha,
eta = eta,
compute.log.likelihood=TRUE
)
lda_results <- top.topic.words(test$topics,10)
as.data.frame(lda_results)
issue_results=slda.em(documents=docs[[1]],
K=k,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha,
eta=eta,
annotations=dat$issue_area,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
top_issue <- top.topic.words(issue_results$topics,10)
as.data.frame(lda_results)
issue_results=slda.em(documents=docs[[1]],
K=k,
vocab=vocab,
num.e.iterations=10,
num.m.iterations=4,
alpha=alpha,
eta=eta,
annotations=dat$issue_area,
params,
variance=0.25,
lambda=1.0,
logistic=FALSE,
method="sLDA"
)
top_issue <- top.topic.words(issue_results$topics,10)
as.data.frame(lda_results)
stm_results = stm(documents=outs_stm$documents,
vocab=outs_stm$vocab,
K=k,
prevalence=
~issue_area+term,
data=outs_stm$meta)
# view 3 top words associated with each topic
plot(stm_results)
stm_results = stm(documents=outs_stm$documents,
vocab=outs_stm$vocab,
K=k,
prevalence=
~issue_area #+term,
data=outs_stm$meta)
stm_results = stm(documents=outs_stm$documents,
vocab=outs_stm$vocab,
K=k,
prevalence=
~issue_area,
#+term,
data=outs_stm$meta)
# view 3 top words associated with each topic
plot(stm_results)
