---
title: "PLSC 468 Final Project"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/6/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r}
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
```


```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)



# load libraries
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(glmnet)
library(caret)
library(ldatuning)
library(stm)
#library(reticulate)
#library(rjson)
#library(jsonlite)
#py_install("pandas")
```

# Load Data

Setting the seed for future randomization:

```{r}
set.seed(1005)
```

```{r}
# court data source: https://github.com/smitp415/CSCI_544_Final_Project

raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) #

dat <- raw

# justice data source: https://mqscores.lsa.umich.edu/measures.php
#justices <- read.csv("final_project/justices.csv")
#chelsea code
justices <- read.csv("justices.csv")
```

# Data Structure

```{r}
dim(dat)
```

We note 16 variables recorded for 3,303 SCOTUS cases. We remove any rows that have missing data, leaving us with 3101 observations. Only some of these variables are relevant for our analysis:

```{r}
dat <- dat %>%
  select(name, href, term, facts, majority_vote, minority_vote, decision_type, disposition, issue_area) %>%
  mutate(term = case_when(term == "1789-1850" ~ round(mean(c(1789, 1850)), 0),
                   term == "1850-1900" ~ round(mean(c(1850, 1900)), 0),
                   term == "1900-1940" ~ round(mean(c(1900, 1940)), 0),
                   term == "1940-1955" ~ round(mean(c(1940, 1955)), 0),
                   TRUE ~ as.numeric(term))) %>%
  mutate(term = as.numeric(term),
         decision_type = as.factor(decision_type),
         disposition = as.factor(disposition),
         issue_area = as.factor(issue_area))

dat <- dat %>%
  na.omit()
```

-name (character): The name of the case, e.g. "Roe v. Wade."

-href (character): The URL link to the case in the Oyez API, for ease of reference

-term (numeric): Since 1955, the term of the Supreme Court begins from October of the given year and extends til October of the following year. Previously, the term of the court has been classified into broader terms: 1789-1850, 1850-1900, 1900-1940, 1940-1955. For our analysis, we want to consider "term" as a continuous variable. Therefore, the ranged values will be replaced with the year in the middle of the range. While it would be preferred to have the actual year of the decision, we note that very few cases take place before 1955: only 59 of the over 3k cases.

-facts (character): This is the raw text of the case facts and needs to be cleaned for future analysis.

-majority_vote (numeric): The number of justices who agreed to the case's disposition

-minority_vote (numeric): The number of justices who did not agree to the case's disposition

-decision_type (factor): A phrase indicating the court's level of agreement on the case's outcome, e.g. "majority opinion," or "dismissal - moot"

-disposition (factor): A phrase indicating the court's judgement on the status of the case, e.g. "reversed" or "affirmed"

-issue_area (factor): A phrase indicating the topic of the case, e.g. "Civil Rights"

To ensure the dataset variables are as expected, we investigate the range of each variable.

```{r}
sum(duplicated(dat))
```

We note no duplicated entries in the dataset.

```{r}
range(dat$term)

ggplot(dat, aes(x=term)) +
  geom_histogram(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Term") +
  ylab("Count") +
  scale_x_continuous(breaks=seq(1800, 2100, 50)) +
  scale_y_continuous(breaks=seq(0, 350, 100), limits=c(0, 250), expand=c(0,0)) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 15),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

sum(dat$term <= 1955)

dat <- dat %>%
  filter(term > 1955)
```

The terms from the dataset range from 1820 to 2020. We note the vast majority of cases in the dataset are from after 1950: only 59 out of the over 3k cases are from earlier. Therefore, we decide to cutoff the dataset at cases after 1955, with the most recent cases being from the term ending in October of 2021.

```{r}
votes <- dat %>%
  select(majority_vote, minority_vote)

votes <- gather(votes, condition, measurement)

ggplot(votes, aes(x=measurement, fill=condition)) +
  geom_histogram() +
  theme_minimal() +
  xlab("Justices") +
  ylab("Count") +
  scale_x_continuous(breaks=seq(0, 9, 1)) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 15),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

sumMajority <- dat %>%
  group_by(term) %>%
  summarize(median_majority = median(majority_vote, na.rm=TRUE),
            mean_majority = mean(majority_vote, na.rm=TRUE))

ggplot(dat, aes(x=term, y=majority_vote)) +
  geom_point(position = "jitter", alpha = .2) +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  geom_line(data = sumMajority, aes(y=median_majority), color="red") +
  geom_line(data = sumMajority, aes(y=mean_majority), color="blue") +
  annotate(geom = "label", x = 1960, y = 8.4, label = "Median", color = "red", size = 3) +
  annotate(geom = "label", x = 1960, y = 7.5, label = "Mean", color = "blue", size = 3) +
  ylab("Majority Votes") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note that majority vote opinions generally occur when 5-9 justices agree on the opinion, leaving a minority of 0-4 justices dissenting. We also note that the most common condition is consensus, when 9 of the justices are on the majority vote and 0 justices are on the minority vote. 

```{r}
table(dat$majority_vote)
```

```{r}
ggplot(dat, aes(x=decision_type)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Decision Type") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```
We note the vast majority of the Court's decisions are majority opinions, where more than half of the Court agrees on the decision and the reasoning. A plurality opinion has the greatest number of votes but not necessarily a majority and is not binding. These opinions typically have 1 justice as the author. In contrast, per curiam opinions do not identify an author and are opinions of the Court.

```{r}
ggplot(dat, aes(x=disposition)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Disposition") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

affirmRemand <- dat %>%
  filter(disposition == "affirmed" | disposition == "reversed") %>%
  group_by(disposition, term) %>%
  summarise(count = n())

ggplot(affirmRemand, aes(x=term, y=count, group=disposition, color=disposition)) +
  geom_line()+
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note a variety of decision types: interestingly, the court affirms a decision about as often as it reverses/remands a decision.

```{r}
ggplot(dat, aes(x=issue_area)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Issue Area") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

issueArea <- dat %>%
  group_by(issue_area, term) %>%
  summarise(count = n())

ggplot(issueArea, aes(x=term, y=count, group=issue_area, color=issue_area)) +
  geom_line()+
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note a variety of issue areas, particularly Civil Rights, Criminal Procedure, and Economic Activity.

Given the large influence the ideological leanings of individual justices is thought to have over court opinions, we combine our main dataset about SCOTUS court cases with another dataset about the Martin-Quinn scores of the justices. A more negative Martin-Quinn score indicates a more liberal record, while a more positive Martin-Quinn score indicates a more conservative record. Martin-Quinn scores may change for a justice from term to term because they are updated as opinions are released. For our purposes, we average the ideological scores of each justice who was on the Court in a given term. 

```{r}
sumMQ <- justices %>%
  group_by(term) %>%
  summarize(median_mq = median(post_mn, na.rm=TRUE),
            mean_mq = mean(post_mn, na.rm=TRUE))

ggplot(justices, aes(x=term)) +
  geom_line(aes(y=post_mn, group=justice)) +
  geom_line(data = sumMQ, aes(y=median_mq), color="red") +
    geom_line(data = sumMQ, aes(y=mean_mq), color="blue") +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Martin-Quinn Score by Justice") +
  annotate(geom = "text", x = 2007, y = 1, label = "Median", color = "red", size = 3) +
  annotate(geom = "text", x = 2007, y = -0.4, label = "Median", color = "blue", size = 3) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

dat <- inner_join(dat, sumMQ[,c(1:2)], by="term")
```

# Text Cleaning

We remove the HTML tags from the case text.

```{r}
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
```

We convert the facts into a corpus, use tm_map() to clean, and create a term-document matrix.

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))

# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))

# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
```

# Most Frequent Words (Kelly)

## Across Entire Corpus

```{r}
m <- as.matrix(tdm)
# reorganizing the matrix to get the most common words
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)

# note: consider removing some of these most common words

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(2, "Paired"))

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```

## By Decade

We track the 10 most common words by decade from the 1960s to the 2010s.

```{r}
mostfreqwords <- d[1:10, 1]

dec60 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1970 & dat$term >= 1960)]))
v_60 <- sort(rowSums(dec60),decreasing=TRUE)
d_60 <- data.frame(word = names(v_60),freq=v_60)

dec70 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1980 & dat$term >= 1970)]))
v_70 <- sort(rowSums(dec70),decreasing=TRUE)
d_70 <- data.frame(word = names(v_70),freq=v_70)

dec80 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1990 & dat$term >= 1980)]))
v_80 <- sort(rowSums(dec80),decreasing=TRUE)
d_80 <- data.frame(word = names(v_80),freq=v_80)

dec90 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2000 & dat$term >= 1990)]))
v_90 <- sort(rowSums(dec90),decreasing=TRUE)
d_90 <- data.frame(word = names(v_90),freq=v_90)

dec00 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2010 & dat$term >= 2000)]))
v_00 <- sort(rowSums(dec00),decreasing=TRUE)
d_00 <- data.frame(word = names(v_00),freq=v_00)

dec10 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2020 & dat$term >= 2010)]))
v_10 <- sort(rowSums(dec10),decreasing=TRUE)
d_10 <- data.frame(word = names(v_10),freq=v_10)

# merge all data frames together
decade_list <- list(d_70, d_80, d_90, d_00, d_10)      
decades <- decade_list %>% reduce(full_join, by="word")
names(decades) <- c("word", "1970s", "1980s", "1990s", "2000s", "2010s")

# calculate relative frequencies by dividing by number of cases
decades_relative <- decades
decades_relative$`1970s` <- decades$`1970s` / sum(na.omit(decades$`1970s`))
decades_relative$`1980s` <- decades$`1980s` / sum(na.omit(decades$`1980s`))
decades_relative$`1990s` <- decades$`1990s` / sum(na.omit(decades$`1990s`))
decades_relative$`2000s` <- decades$`2000s` / sum(na.omit(decades$`2000s`))
decades_relative$`2010s` <- decades$`2010s` / sum(na.omit(decades$`2010s`))

# most frequent
decades_relative_top <- decades_relative %>%
  filter(word %in% mostfreqwords) %>%
  pivot_longer(cols = c(2:6)) %>%
  mutate(name = as.numeric(substr(name, 1, 4))) %>%
  rename(year = name,
         proportion = value)

ggplot(decades_relative_top, aes(x=year, y=proportion, group=word, color = word)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1970, 2020, 10)) +
  ylab("Frequency of Word") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

## By Issue Area

We choose to examine the top 3 issue areas: Civil Rights, Criminal Procedure, and Economic Activity.

```{r}
civilRights <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Civil Rights")]))
v_cr <- sort(rowSums(civilRights),decreasing=TRUE)
d_cr <- data.frame(word = names(v_cr),freq=v_cr)

barplot(d_cr[1:10,]$freq, las = 2, names.arg = d_cr[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Civil Rights",
        ylab = "Word frequencies")


criminalProcedure <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Criminal Procedure")]))
v_cp <- sort(rowSums(criminalProcedure),decreasing=TRUE)
d_cp <- data.frame(word = names(v_cp),freq=v_cp)


barplot(d_cp[1:10,]$freq, las = 2, names.arg = d_cp[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Criminal Procedure",
        ylab = "Word frequencies")


economicActivity <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Economic Activity")]))
v_ea <- sort(rowSums(economicActivity),decreasing=TRUE)
d_ea <- data.frame(word = names(v_ea),freq=v_ea)


barplot(d_ea[1:10,]$freq, las = 2, names.arg = d_ea[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Economic Activity",
        ylab = "Word frequencies")
```

# Topic Modeling (Numi)



Because we are interesting in the variation in how different words, issues, are linked to the level of Supreme Court disagreement, we now turn to structural topic models.

Our first task is determining the number of topics emerge in our data set. To do so, we first prepare the Supreme Court corpus for analysis. Originally, we used the full data set for STM models; however, because there is a are words with high frequency that do not meaningfully improve our understanding of a case (for example, neutral procedural terms) we implement a word frequency threshold. Here, our baseline models eliminate words that appear in more than 50% of documents. 


#STM Prep
```{r}
#stm prep
temp = textProcessor(documents=dat$facts,metadata=dat[,1:10])
outs_stm= prepDocuments(temp$documents, temp$vocab, temp$meta, upper.thresh = 773)
```

Before running the model, we also need to determine the number of topics for the models to predict. In determining the value of k, we aim to minimize the Arun and CaoJuan metrics and maximize the Deveaud and Grifiths metrics. 

```{r}
# creating a tdm of documents
temp = Corpus(VectorSource(outs_stm$documents))
tdm = DocumentTermMatrix(temp)

# calculating lda tuning metrics - big picture overview
metrics <- FindTopicsNumber(
  tdm,
  topics = seq(from = 2, to = 200, by = 20),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics)

# calculating lda tuning metrics - 5-30

metrics_final <- FindTopicsNumber(
  tdm,
  topics = seq(from = 5, to = 30, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(metrics_final)
```
To do so, we first begin with a wide set of possible K values (2-100) to understand the broad pattern.It appears that the Arun, CaoJuan, and Deveaud metrics decrease over time while the Griffiths metric increases over time. 

After confirming that there are no anomalous patterns as K reaches higher values, we then narrow our value range significantly. After fifteen, there begins to be a divergence in the Deveaud and Griffiths metrics. Looking below 15, the CaoJuan measure is minimized at k=12 and the Arun metric steadily decreases. As such, we set k=12 for the rest of the models. 

```{r}
k <- 6
```

#STM 
Next, we look to structural topic models (STM). We begin with a "baseline" models which uses just issue area and term to predict groupings. For each model we plot the three most frequent words in each topic and additionally generate the whole list of top ten words for each of the topics. 

```{r}
stm_results = stm(documents=outs_stm$documents,
                  vocab=outs_stm$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term,
                  data=outs_stm$meta)

plot(stm_results)

stm_results <- t(labelTopics(model=stm_results, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_results) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results)
```

Next, we proceed by adding a series of covariates to our topic models. 

Since we are interested in understanding the effect of party polarization over time, our next model uses the median MQ score as a covariate in predicting topics.

MQ scores 
```{r}
stm_results_mq <- stm(documents=outs_stm$documents,
                  vocab=outs_stm$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq,
                  data=outs_stm$meta)

plot(stm_results_mq)

stm_results_mq <- t(labelTopics(model=stm_results_mq, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results_mq) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_results_mq) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results_mq)
```

Now we investigate how this full set of covariates which control for 

Disposition  
```{r}
stm_results_disposition = stm(documents=outs_stm$documents,
                  vocab=outs_stm$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+ median_mq + disposition ,
                  data=outs_stm$meta)

plot(stm_results_disposition)

stm_results_disposition <- t(labelTopics(model=stm_results_disposition, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results_disposition) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_results_disposition) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results_disposition)
```

Majority Vote 
```{r}
stm_results_majority = stm(documents=outs_stm$documents,
                  vocab=outs_stm$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq + majority_vote ,
                  data=outs_stm$meta)

plot(stm_results_majority)

stm_results_majority <- t(labelTopics(model=stm_results_majority, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_results_majority) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_results_majority) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_results_majority)
```

Next, we are interested in testing more stringent cutoffs on our majority votes model. To do so we rerun the above analysis with a .5 and .75 cut off. 

```{r}
temp = textProcessor(documents=dat$facts,metadata=dat[,1:10])
outs_stm_50= prepDocuments(temp$documents, temp$vocab, temp$meta, upper.thresh = 1550)

stm_majority_50 = stm(documents=outs_stm_50$documents,
                  vocab=outs_stm_50$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq + majority_vote ,
                  data=outs_stm_50$meta)

plot(stm_majority_50)

stm_majority_50 <- t(labelTopics(model=stm_majority_50, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_majority_50) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_majority_50) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_majority_50)

temp = textProcessor(documents=dat$facts,metadata=dat[,1:10])
outs_stm_75= prepDocuments(temp$documents, temp$vocab, temp$meta, upper.thresh = 2325)

stm_majority_75 = stm(documents=outs_stm_75$documents,
                  vocab=outs_stm_75$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq +majority_vote,
                  data=outs_stm_75$meta)

plot(stm_majority_75)

stm_majority_75 <- t(labelTopics(model=stm_majority_75, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(stm_majority_75) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(stm_majority_75) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(stm_majority_75)

```


Lastly, we are interested in seeing whether these trends vary over time. In particular, it is popularly claimed that in the last twenty years, the court has become increasingly politicized. We now divide the dataset into two periods, cases heard from 1955-2009 and those heard after 2009. We chose this cutoff date as this is when the number of Senate confirmation votes for the average SCOTUS appointee dropped significantly. In this section, we compare topics generated during each time period to see whether topics vary differently with time. We note that the sample size for this analysis is smaller than before. The pre-2009 dataset has 2330 observations whereas the post-2009 has only 765. 

To begin, we divide the dataset into the groups defined above. We then conduct the same data preprocessing techniques as above, using a 50% word frequency threshold. 
```{r}
dat_pre <- dat %>%
  filter(term <=2009)

temp = textProcessor(documents=dat_pre$facts,metadata=dat_pre[,1:10])
outs_stm_pre= prepDocuments(temp$documents, temp$vocab, temp$meta, upper.thresh = 500)

dat_post <- dat %>% 
  filter(term > 2009)

temp = textProcessor(documents=dat_post$facts,metadata=dat_post[,1:10])
outs_stm_post= prepDocuments(temp$documents, temp$vocab, temp$meta, upper.thresh = 500)
```

#MQ Scores:

```{r}
pre_results_mq <- stm(documents=outs_stm_pre$documents,
                  vocab=outs_stm_pre$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq,
                  data=outs_stm_pre$meta)

plot(pre_results_mq)

pre_results_mq <- t(labelTopics(model=pre_results_mq, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(pre_results_mq) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(pre_results_mq) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(pre_results_mq)

post_results_mq <- stm(documents=outs_stm_post$documents,
                  vocab=outs_stm_post$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq,
                  data=outs_stm_post$meta)

plot(post_results_mq)

post_results_mq <- t(labelTopics(model=post_results_mq, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(post_results_mq) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6")
rownames(post_results_mq) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(post_results_mq)
```

Next, we turn to variation in majority votes over time by topic. 
```{r}
pre_results_maj <- stm(documents=outs_stm_pre$documents,
                  vocab=outs_stm_pre$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq+majority_vote,
                  data=outs_stm_pre$meta)

plot(pre_results_maj)

pre_results_maj <- t(labelTopics(model=pre_results_maj, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(pre_results_maj) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11", "Topic 12")
rownames(pre_results_maj) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(pre_results_maj)

post_results_maj <- stm(documents=outs_stm_post$documents,
                  vocab=outs_stm_post$vocab,
                  K=k,
                  prevalence=
                    ~issue_area+term+median_mq+majority_vote,
                  data=outs_stm_post$meta)

plot(post_results_maj)

post_results_maj <- t(labelTopics(model=post_results_maj, topics = NULL, n = 10, frexweight = 0.5)[1]$prob)

colnames(post_results_maj) <- c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6", "Topic 7", 
                          "Topic 8", "Topic 9", "Topic 10", "Topic 11", "Topic 12")
rownames(post_results_maj) <- c("#1", "#2", "#3", "#4", "#5", "#6" , "#7", "#8", "9", "#10")
as.data.frame(post_results_maj)
```


# Elastic Net Regression


## Model 1: Predicting the number of justices on the majority opinion

First, we subset our data to variables of interest and we clean the corpus of case fact data.

```{r}
# subsetting to complete data and selecting relevant variables
x_raw <- na.omit(subset(dat, select = -c(name, href, minority_vote)))
names(x_raw)[1] <- "jterm"
y <- x_raw$majority_vote
```

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(x_raw$facts))

# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
```

Next, we remove words that appear in over 40% of documents.

```{r}
# removing frequent words
ndocs <- length(corpus)
minDocFreq <- ndocs * 0.01
maxDocFreq <- ndocs * 0.4 # 50% as upper - model was kind of struggling w higher bounds

# creating a dtm 
dtm <- DocumentTermMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
dtm.mat <- as.matrix(dtm)

# combining dtm with other data
x <- subset(x_raw, select = -c(majority_vote, facts))
x <- cbind(x, dtm.mat)
```

Next, we split our data 50/50 into test and trainig sets, and we tune an optimal alpha and lambda based on our training data. For this section, the range of lambda values is quite small, as lambda ranges any larger resulted in selection of a large lambda and elimination of all predictors. 

```{r}
set.seed(1005)

# split into test/training
N=nrow(x)
s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
  
# train with 1/2 of known
x.train <- x[which(s.vec==1),]
y.train <- y[which(s.vec==1)]
  
# test with other 1/2 of known
x.test <- x[which(s.vec==2),]
y.test <- y[which(s.vec==2)]
```

```{r}
set.seed(1005)

# initial parameters
k=10
alpha=seq(from=0,to=1,by=0.1)
lambda=seq(from=0.1,to=0.3,by=0.05) 

# tuning alpha and lambda
sumMSE=matrix(NA,length(alpha),length(lambda))

for(i in 1:length(alpha)){
  oo=cv.glmnet(nfolds=k,x=data.matrix(x.train), y=data.matrix(y.train), nlambda=length(lambda),
  	alpha=alpha[i],lambda=lambda, standardize = T)
  if(length(oo$cvm)==length(sumMSE[i,])){
  	sumMSE[i,]=oo$cvm
  }
}

# identifiying alpha and lamda that minimizes MSE
for(i in 1:length(alpha)){
  if(length(which(sumMSE[i,]==min(sumMSE)))>0){break}
}

alpha[i]
lambda[which(sumMSE[i,]==min(sumMSE))]
```

We next create a final model based on our optimized alpha and lambda values.

```{r}
# run final optimized model
oo=glmnet(x=x.train, y=y.train, family=c("gaussian"), 
  	alpha=alpha[i],lambda=lambda[which(sumMSE[i,]==min(sumMSE))])

# which are the selected covariates
round(oo$beta[,1][which(oo$beta[,1]!=0)],digits=5)
length(which(oo$beta[,1]!=0))
```

Using our final model, we evaluate its performance on our test data and calculate the number of cases where the number of justices on the majority opinion was predicted correctly. 

```{r}
# predictions on the training and test sets
y_train_hat2 = predict(oo, data.matrix(x.train))
y_test_hat2 <- predict(oo, data.matrix(x.test))

# calculating accuracy on test data
ydiff <- round(y_test_hat2) - y.test
accuracy <- sum(ydiff == 0)/length(ydiff)

# classification table on test data
table(round(y_test_hat2), y.test)
```


## Model 2: Predicting if a case was contested or uncontested

We use the same predictors as model 1, but we create a binary variable instead, using the number of justices on the majority opinion to classify cases as either contested or non-contested. We argue that 7 or more justices represent an uncontested case, while 5 or 6 justices represent a contested case. Here, we are being conservative with what we consider to be a contested case.

```{r}
# creating a binary variable, 1 = agree, -1 = contested
agree <- ifelse(y >= 7, 1, -1)

# splitting test and training 
agree.train <- agree[which(s.vec==1)]
agree.test <- agree[which(s.vec==2)]
```

# We tune an optimal lambda and alpha, run a final model, and create classification tables and calculate accuracy, following a similar procedure as Model 1.

```{r}
set.seed(1005)

# initial parameters
k=10
alpha=seq(from=0,to=1,by=0.1)
lambda=seq(from=0.01,to=0.4,by=0.05) 

# tuning alpha and lambda
sumMSE=matrix(NA,length(alpha),length(lambda))

for(i in 1:length(alpha)){
  oo=cv.glmnet(nfolds=k,x=data.matrix(x.train), y=data.matrix(agree.train), nlambda=length(lambda),
  	alpha=alpha[i],lambda=lambda, standardize = T)
  if(length(oo$cvm)==length(sumMSE[i,])){
  	sumMSE[i,]=oo$cvm
  }
}

# identifiying alpha and lamda that minimizes MSE
for(i in 1:length(alpha)){
  if(length(which(sumMSE[i,]==min(sumMSE)))>0){break}
}

alpha[i]
lambda[which(sumMSE[i,]==min(sumMSE))]
```

```{r}
# run final optimized model
oo=glmnet(x=x.train, y=agree.train, family=c("gaussian"),
  	alpha=alpha[i],lambda=lambda[which(sumMSE[i,]==min(sumMSE))])

# which are the selected covariates
round(oo$beta[,1][which(oo$beta[,1]!=0)],digits=5)
length(which(oo$beta[,1]!=0))
```

```{r}
# classification table on training data
prd.train=predict(oo,data.matrix(x.train))
tau=(min(prd.train[agree.train==-1])+max(prd.train[agree.train==1]))/2
y.test <- as.numeric(prd.train>tau)
y.test[y.test == 0] <- -1
table(agree.train,y.test)

# classification table on test data
prd.test=predict(oo, data.matrix(x.test))
y.pred <- as.numeric(prd.test>tau)
y.pred[y.pred == 0] <- -1
table(agree.test,y.pred)

# calculate accuracy on test data
sub.vector <- y.pred - agree.test # 0's indicate correct classification
accuracy <- sum(sub.vector == 0) / length(sub.vector)

accuracy
```

## Model 3: Predicting if a case was affirmed or reversed

Next, we create a separate model to predict if a case was affirmed or reversed. First, we subset to cases that either had an affirmed or reversed disposition. Because we create a new set of data, we recreate and reclean the corpus.

```{r}
# subsetting to cases that were affirmed or reversed
x_AR <- dat %>%
  filter(disposition == "affirmed" | disposition == "reversed") 

# subsetting to data of interest
x_raw_AR <- na.omit(subset(x_AR, select = -c(name, href, minority_vote, disposition)))
names(x_raw_AR)[1] <- "jterm"
AR_raw <- x_AR$disposition
```

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(x_raw_AR$facts))

# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
```

We again remove words that appear in over 40% of documents in the corpus.

```{r}
# removing frequent words
ndocs <- length(corpus)
minDocFreq <- ndocs * 0.01
maxDocFreq <- ndocs * 0.4 # 50% as upper - model was kind of struggling w higher bounds

# creating a dtm 
dtm <- DocumentTermMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
dtm.mat <- as.matrix(dtm)

# combining dtm with other data
x <- subset(x_raw_AR, select = -c(majority_vote, facts))
x <- cbind(x, dtm.mat)
```

We split our data 50/50 into training/test data and create a binary variable to represent affirmed/reversed.

```{r}
set.seed(1005)
# split into test/training
N=nrow(x)
s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
  
# train with 1/2 of known
x.train <- x[which(s.vec==1),]
  
# test with other 1/2 of known
x.test <- x[which(s.vec==2),]
```

```{r}
# creating a binary variable, 1 = affirmed, -1 = remanded
AR <- ifelse(AR_raw == "affirmed", 1, -1)

# splitting test and training 
AR.train <- AR[which(s.vec==1)]
AR.test <- AR[which(s.vec==2)]
```

Next, following a similar procedure as Models 1 and 2, we tune alpha and lambda, run an optimized model on our training data, and determine the accuracy of the model in classifying our test data.

```{r}
set.seed(1005)

# initial parameters
k=10
alpha=seq(from=0,to=1,by=0.1)
lambda=seq(from=0.1,to=1.5,by=0.1) # had to set a very small lambda range, otherwise all predictors are removed

# tuning alpha and lambda
sumMSE=matrix(NA,length(alpha),length(lambda))

for(i in 1:length(alpha)){
  oo=cv.glmnet(nfolds=k,x=data.matrix(x.train), y=data.matrix(AR.train), nlambda=length(lambda),
  	alpha=alpha[i],lambda=lambda, standardize = T)
  if(length(oo$cvm)==length(sumMSE[i,])){
  	sumMSE[i,]=oo$cvm
  }
}

# identifiying alpha and lamda that minimizes MSE
for(i in 1:length(alpha)){
  if(length(which(sumMSE[i,]==min(sumMSE)))>0){break}
}

alpha[i]
lambda[which(sumMSE[i,]==min(sumMSE))]
```

```{r}
# run final optimized model
oo=glmnet(x=x.train, y=AR.train, family=c("gaussian"),
  	alpha=alpha[i],lambda=lambda[which(sumMSE[i,]==min(sumMSE))])

# which are the selected covariates
round(oo$beta[,1][which(oo$beta[,1]!=0)],digits=5)
length(which(oo$beta[,1]!=0))
```

```{r}
# creating classification tables on training data
prd.train=predict(oo,data.matrix(x.train))
tau=(min(prd.train[AR.train==-1])+max(prd.train[AR.train==1]))/2
y.test <- as.numeric(prd.train>tau)
y.test[y.test == 0] <- -1
table(AR.train,y.test)

# creating classification tables on test data
prd.test=predict(oo, data.matrix(x.test))
y.pred <- as.numeric(prd.test>tau)
y.pred[y.pred == 0] <- -1
table(AR.test,y.pred)

# calculate accuracy on test data
sub.vector <- y.pred - AR.test # 0's indicate correct classification
accuracy <- sum(sub.vector == 0) / length(sub.vector)

accuracy
```

