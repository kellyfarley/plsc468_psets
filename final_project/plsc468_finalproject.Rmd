---
title: "PLSC 468 Final Project"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/6/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r}
# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)
```


```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)



# load libraries
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(glmnet)
library(caret)
#library(reticulate)
#library(rjson)
#library(jsonlite)
#py_install("pandas")
```

# Load Data

Setting the seed for future randomization:

```{r}
set.seed(1005)
```

```{r}
# court data source: https://github.com/smitp415/CSCI_544_Final_Project

raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) #

dat <- raw

# justice data source: https://mqscores.lsa.umich.edu/measures.php
#justices <- read.csv("final_project/justices.csv")
#chelsea code
justices <- read.csv("justices.csv")
```

# Data Structure

```{r}
dim(dat)
```

We note 16 variables recorded for 3,303 SCOTUS cases. Only some of these variables are relevant for our analysis:

```{r}
dat <- dat %>%
  select(name, href, term, facts, majority_vote, minority_vote, decision_type, disposition, issue_area) %>%
  mutate(term = case_when(term == "1789-1850" ~ round(mean(c(1789, 1850)), 0),
                   term == "1850-1900" ~ round(mean(c(1850, 1900)), 0),
                   term == "1900-1940" ~ round(mean(c(1900, 1940)), 0),
                   term == "1940-1955" ~ round(mean(c(1940, 1955)), 0),
                   TRUE ~ as.numeric(term))) %>%
  mutate(term = as.numeric(term),
         decision_type = as.factor(decision_type),
         disposition = as.factor(disposition),
         issue_area = as.factor(issue_area))
```

-name (character): The name of the case, e.g. "Roe v. Wade."

-href (character): The URL link to the case in the Oyez API, for ease of reference

-term (numeric): Since 1955, the term of the Supreme Court begins from October of the given year and extends til October of the following year. Previously, the term of the court has been classified into broader terms: 1789-1850, 1850-1900, 1900-1940, 1940-1955. For our analysis, we want to consider "term" as a continuous variable. Therefore, the ranged values will be replaced with the year in the middle of the range. While it would be preferred to have the actual year of the decision, we note that very few cases take place before 1955: only 59 of the over 3k cases.

-facts (character): This is the raw text of the case facts and needs to be cleaned for future analysis.

-majority_vote (numeric): The number of justices who agreed to the case's disposition

-minority_vote (numeric): The number of justices who did not agree to the case's disposition

-decision_type (factor): A phrase indicating the court's level of agreement on the case's outcome, e.g. "majority opinion," or "dismissal - moot"

-disposition (factor): A phrase indicating the court's judgement on the status of the case, e.g. "reversed" or "affirmed"

-issue_area (factor): A phrase indicating the topic of the case, e.g. "Civil Rights"

To ensure the dataset variables are as expected, we investigate the range of each variable.

```{r}
sum(duplicated(dat))
```

We note no duplicated entries in the dataset.

```{r}
range(dat$term)

ggplot(dat, aes(x=term)) +
  geom_histogram(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Term") +
  ylab("Count") +
  scale_x_continuous(breaks=seq(1800, 2100, 50)) +
  scale_y_continuous(breaks=seq(0, 350, 100), limits=c(0, 250), expand=c(0,0)) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 15),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

sum(dat$term <= 1955)

dat <- dat %>%
  filter(term > 1955)
```

The terms from the dataset range from 1820 to 2020. We note the vast majority of cases in the dataset are from after 1950: only 59 out of the over 3k cases are from earlier. Therefore, we decide to cutoff the dataset at cases after 1955, with the most recent cases being from the term ending in October of 2021.

```{r}
votes <- dat %>%
  select(majority_vote, minority_vote)

votes <- gather(votes, condition, measurement)

ggplot(votes, aes(x=measurement, fill=condition)) +
  geom_histogram() +
  theme_minimal() +
  xlab("Justices") +
  ylab("Count") +
  scale_x_continuous(breaks=seq(0, 9, 1)) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 15),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

sumMajority <- dat %>%
  group_by(term) %>%
  summarize(median_majority = median(majority_vote, na.rm=TRUE),
            mean_majority = mean(majority_vote, na.rm=TRUE))

ggplot(dat, aes(x=term, y=majority_vote)) +
  geom_point(position = "jitter", alpha = .2) +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  geom_line(data = sumMajority, aes(y=median_majority), color="red") +
  geom_line(data = sumMajority, aes(y=mean_majority), color="blue") +
  annotate(geom = "label", x = 1960, y = 8.4, label = "Median", color = "red", size = 3) +
  annotate(geom = "label", x = 1960, y = 7.5, label = "Mean", color = "blue", size = 3) +
  ylab("Majority Votes") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note that majority vote opinions generally occur when 5-9 justices agree on the opinion, leaving a minority of 0-4 justices dissenting. We also note that the most common condition is consensus, when 9 of the justices are on the majority vote and 0 justices are on the minority vote. 

```{r}
table(dat$majority_vote)
```

```{r}
ggplot(dat, aes(x=decision_type)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Decision Type") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```
We note the vast majority of the Court's decisions are majority opinions, where more than half of the Court agrees on the decision and the reasoning. A plurality opinion has the greatest number of votes but not necessarily a majority and is not binding. These opinions typically have 1 justice as the author. In contrast, per curiam opinions do not identify an author and are opinions of the Court.

```{r}
ggplot(dat, aes(x=disposition)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Disposition") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

affirmRemand <- dat %>%
  filter(disposition == "affirmed" | disposition == "reversed") %>%
  group_by(disposition, term) %>%
  summarise(count = n())

ggplot(affirmRemand, aes(x=term, y=count, group=disposition, color=disposition)) +
  geom_line()+
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note a variety of decision types: interestingly, the court affirms a decision about as often as it reverses/remands a decision.

```{r}
ggplot(dat, aes(x=issue_area)) +
  geom_bar(fill="#8D9DD5", color="#E3EBFF") +
  theme_minimal() +
  xlab("Issue Area") +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

issueArea <- dat %>%
  group_by(issue_area, term) %>%
  summarise(count = n())

ggplot(issueArea, aes(x=term, y=count, group=issue_area, color=issue_area)) +
  geom_line()+
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Count") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

We note a variety of issue areas, particularly Civil Rights, Criminal Procedure, and Economic Activity.

Given the large influence the ideological leanings of individual justices is thought to have over court opinions, we combine our main dataset about SCOTUS court cases with another dataset about the Martin-Quinn scores of the justices. A more negative Martin-Quinn score indicates a more liberal record, while a more positive Martin-Quinn score indicates a more conservative record. Martin-Quinn scores may change for a justice from term to term because they are updated as opinions are released. For our purposes, we average the ideological scores of each justice who was on the Court in a given term. 

```{r}
sumMQ <- justices %>%
  group_by(term) %>%
  summarize(median_mq = median(post_mn, na.rm=TRUE),
            mean_mq = mean(post_mn, na.rm=TRUE))

ggplot(justices, aes(x=term)) +
  geom_line(aes(y=post_mn, group=justice)) +
  geom_line(data = sumMQ, aes(y=median_mq), color="red") +
    geom_line(data = sumMQ, aes(y=mean_mq), color="blue") +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1950, 2050, 10)) +
  ylab("Martin-Quinn Score by Justice") +
  annotate(geom = "text", x = 2007, y = 1, label = "Median", color = "red", size = 3) +
  annotate(geom = "text", x = 2007, y = -0.4, label = "Median", color = "blue", size = 3) +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )

dat <- inner_join(dat, sumMQ[,c(1:2)], by="term")
```

# Text Cleaning

We remove the HTML tags from the case text.

```{r}
dat$facts <- gsub("<[^<>]*>", "", dat$facts)
dat$facts <- gsub("\t", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("\n", "", dat$facts, fixed = TRUE)
dat$facts <- gsub("&#82[0-9]{2};", "", dat$facts)
```

We convert the facts into a corpus, use tm_map() to clean, and create a term-document matrix.

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(dat$facts))

# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))

# remove overly sparse (less than 1% of docs) and overly common (more than 80% of docs)
#ndocs <- length(corpus)
# ignore overly sparse terms (appearing in less than 1% of the documents)
#minDocFreq <- ndocs * 0.01
# ignore overly common terms (appearing in more than 80% of the documents)
#maxDocFreq <- ndocs * 0.8
#tdm <- TermDocumentMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))

# creating a term document matrix
tdm <- TermDocumentMatrix(corpus)
```

# Most Frequent Words (Kelly)

## Across Entire Corpus

```{r}
m <- as.matrix(tdm)
# reorganizing the matrix to get the most common words
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)

# note: consider removing some of these most common words

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(2, "Paired"))

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```

## By Decade

We track the 10 most common words by decade from the 1960s to the 2010s.

```{r}
mostfreqwords <- d[1:10, 1]

dec60 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1970 & dat$term >= 1960)]))
v_60 <- sort(rowSums(dec60),decreasing=TRUE)
d_60 <- data.frame(word = names(v_60),freq=v_60)

dec70 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1980 & dat$term >= 1970)]))
v_70 <- sort(rowSums(dec70),decreasing=TRUE)
d_70 <- data.frame(word = names(v_70),freq=v_70)

dec80 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 1990 & dat$term >= 1980)]))
v_80 <- sort(rowSums(dec80),decreasing=TRUE)
d_80 <- data.frame(word = names(v_80),freq=v_80)

dec90 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2000 & dat$term >= 1990)]))
v_90 <- sort(rowSums(dec90),decreasing=TRUE)
d_90 <- data.frame(word = names(v_90),freq=v_90)

dec00 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2010 & dat$term >= 2000)]))
v_00 <- sort(rowSums(dec00),decreasing=TRUE)
d_00 <- data.frame(word = names(v_00),freq=v_00)

dec10 <- as.matrix(TermDocumentMatrix(corpus[which(dat$term < 2020 & dat$term >= 2010)]))
v_10 <- sort(rowSums(dec10),decreasing=TRUE)
d_10 <- data.frame(word = names(v_10),freq=v_10)

# merge all data frames together
decade_list <- list(d_70, d_80, d_90, d_00, d_10)      
decades <- decade_list %>% reduce(full_join, by="word")
names(decades) <- c("word", "1970s", "1980s", "1990s", "2000s", "2010s")

# calculate relative frequencies by dividing by number of cases
decades_relative <- decades
decades_relative$`1970s` <- decades$`1970s` / sum(na.omit(decades$`1970s`))
decades_relative$`1980s` <- decades$`1980s` / sum(na.omit(decades$`1980s`))
decades_relative$`1990s` <- decades$`1990s` / sum(na.omit(decades$`1990s`))
decades_relative$`2000s` <- decades$`2000s` / sum(na.omit(decades$`2000s`))
decades_relative$`2010s` <- decades$`2010s` / sum(na.omit(decades$`2010s`))

# most frequent
decades_relative_top <- decades_relative %>%
  filter(word %in% mostfreqwords) %>%
  pivot_longer(cols = c(2:6)) %>%
  mutate(name = as.numeric(substr(name, 1, 4))) %>%
  rename(year = name,
         proportion = value)

ggplot(decades_relative_top, aes(x=year, y=proportion, group=word, color = word)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  xlab("Term") +
  scale_x_continuous(breaks=seq(1970, 2020, 10)) +
  ylab("Frequency of Word") +
  theme(
    axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    text = element_text(family = "Palatino", size = 10),
    axis.title.y = element_text(face = "bold"),
    axis.title.x = element_text(face = "bold"),
  )
```

## By Issue Area

We choose to examine the top 3 issue areas: Civil Rights, Criminal Procedure, and Economic Activity.

```{r}
civilRights <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Civil Rights")]))
v_cr <- sort(rowSums(civilRights),decreasing=TRUE)
d_cr <- data.frame(word = names(v_cr),freq=v_cr)

barplot(d_cr[1:10,]$freq, las = 2, names.arg = d_cr[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Civil Rights",
        ylab = "Word frequencies")


criminalProcedure <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Criminal Procedure")]))
v_cp <- sort(rowSums(criminalProcedure),decreasing=TRUE)
d_cp <- data.frame(word = names(v_cp),freq=v_cp)


barplot(d_cp[1:10,]$freq, las = 2, names.arg = d_cp[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Criminal Procedure",
        ylab = "Word frequencies")


economicActivity <- as.matrix(TermDocumentMatrix(corpus[which(dat$issue_area == "Economic Activity")]))
v_ea <- sort(rowSums(economicActivity),decreasing=TRUE)
d_ea <- data.frame(word = names(v_ea),freq=v_ea)


barplot(d_ea[1:10,]$freq, las = 2, names.arg = d_ea[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words: Economic Activity",
        ylab = "Word frequencies")
```

# Topic Modeling (Numi)

From Pset3

# Lasso, Ridge, and Elastic Net Regression (Chelsea)

From Pset4

Notes from Chelsea: I'm going to proceed w elastic net - it is computationally most expensive but I think it is useful in that it combines features of Lasso and Ridge - can optimize both alpha and lambda., change size of traning/test

Things to try: just lasso or ridge, classifying rather than predicting the level of agreement


Model 1: predicting the level of agreement (continuous)

```{r}
# subsetting to complete data and selecting variables
# note: minority vote was also removed bc that perfectly predicts majority vote

x <- na.omit(subset(dat, select = -c(name, href, minority_vote)))
names(x)[1] <- "jterm"
y <- x$majority_vote
```

```{r}
# creating a corpus
corpus <- Corpus(VectorSource(x$facts))

# stem using porter algorithm
corpus <- tm_map(corpus, stemDocument)

# remove uppercase
corpus <- tm_map(corpus, tolower)

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove extra white space
corpus <- tm_map(corpus, stripWhitespace)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
```

```{r}
# removing frequent words

ndocs <- length(corpus)
minDocFreq <- ndocs * 0.01
maxDocFreq <- ndocs * 0.5 # 50% as upper - model was kind of struggling w higher bounds

# creating a dtm 
dtm <- DocumentTermMatrix(corpus, control = list(bounds = list(global = c(minDocFreq, maxDocFreq))))
dtm.mat <- as.matrix(dtm)

# combining dtm with other data
x <- subset(x, select = -c(majority_vote, facts))
x <- cbind(x, dtm.mat)
```


```{r}
# split into test/training
N=nrow(x)
s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
  
# train with 1/2 of known
x.train <- x[which(s.vec==1),]
y.train <- y[which(s.vec==1)]
  
# test with other 1/2 of known
x.test <- x[which(s.vec==2),]
y.test <- y[which(s.vec==2)]
```

```{r}
# initial parameters
k=10
alpha=seq(from=0,to=1,by=0.1)
lambda=seq(from=0.1,to=1,by=0.05) # had to set a very small lambda range, otherwise all predictors are removed

# tuning alpha and lambda
sumMSE=matrix(NA,length(alpha),length(lambda))

for(i in 1:length(alpha)){
  oo=cv.glmnet(nfolds=k,x=data.matrix(x.train), y=data.matrix(y.train), nlambda=length(lambda),
  	alpha=alpha[i],lambda=lambda, standardize = T)
  if(length(oo$cvm)==length(sumMSE[i,])){
  	sumMSE[i,]=oo$cvm
  }
}

# identifiying alpha and lamda that minimizes MSE
for(i in 1:length(alpha)){
  if(length(which(sumMSE[i,]==min(sumMSE)))>0){break}
}

alpha[i]
lambda[which(sumMSE[i,]==min(sumMSE))]
```

Testing a package that automatically tunes alpha and lambda
```{r}
# setting the training control
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              search = "random",
                              verboseIter = TRUE)

enet <- train(y.train ~ ., data = cbind(y.train, x.train),
                          method = "glmnet",
                           preProcess = c("center", "scale"),
                           trControl = train_control)
```

```{r}
# predictions on the training and test sets
y_train_hat = predict(enet, x.train)
y_test_hat <- predict(enet, x.test)

# calculating r-squared values
rsq_train_enet = cor(y.train, y_train_hat)^2
rsq_train_enet
rsq_test_enet <- cor(y.test, y_test_hat)^2
rsq_test_enet
```


```{r}
# run final optimized model
oo=glmnet(x=x.train, y=y.train, family=c("gaussian"), 
  	alpha=alpha[i],lambda=lambda[which(sumMSE[i,]==min(sumMSE))])
```

Problem: consistently selecting no predictors, is lambda the issue

```{r}
# which are the selected covariates
round(oo$beta[,1][which(oo$beta[,1]!=0)],digits=5)
length(which(oo$beta[,1]!=0))
```

Below not running bc no predictors

```{r}
# predictions on the training and test sets
y_train_hat2 = predict(oo, x.train)
y_test_hat2 <- predict(oo, x.test)

# calculating r-squared values
rsq_train_enet2 = cor(y.train, y_train_hat2)^2
rsq_train_enet2
rsq_test_enet2 <- cor(y.test, y_test_hat2)^2
rsq_test_enet2
```

```{r}
# train
prd.train=predict(oo,x.train)
tau=(min(prd.train[y.train==-1])+max(prd.train[y.train==1]))/2
table(y.train,as.numeric(prd.train>tau))

# test
prd.test=predict(oo,x.test)
table(y.test,as.numeric(prd.test>tau))

# test
test_1b <- lasso_reg$a0+x.test%*%lasso_reg$beta[,1]
tau=(min(test_1b[y.test==-1])+max(test_1b[y.test==1]))/2

y.pred <- as.numeric(prd.test>tau)
y.pred[y.pred == 0] <- -1

# calculate accuracy
sub.vector <- y.pred - y.test # 0's indicate correct classification
accuracy <- sum(sub.vector == 0) / length(sub.vector)
```


Below: Chelsea testing
```{r}
#creating a term document matrix
tdm <- as.TermDocumentMatrix(t(dtm.mat), weighting = weightTf)

m <- as.matrix(tdm)
# reorganizing the matrix to get the most common words
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20)

# note: consider removing some of these most common words

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(2, "Paired"))

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```


