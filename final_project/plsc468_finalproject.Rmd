---
title: "PLSC 468 Final Project"
author: "Kelly Farley, Numi Katz, and Chelsea Wang"
date: "4/6/2022"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls()) # clear global environ

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)

# set working directory (change for each user)
wd <- "/Users/kellyfarley/Desktop/machine_learning/plsc468_psets"
#wd <- "/Users/numikatz/Documents/Senior_Year/Spring_22/PLSC_468/PLSC_468/plsc468_psets"
knitr::opts_knit$set(root.dir = wd)

# load libraries
library(e1071)
library(stringr)
library(tm)
library(tidyverse)
#library(reticulate)
#library(rjson)
#library(jsonlite)
#py_install("pandas")
```

# Load Data

Setting the seed for future randomization:

```{r}
set.seed(1005)
```

```{r}
# data source: https://github.com/smitp415/CSCI_544_Final_Project
# has multiple data sets...

#all <- fromJSON(file="/Users/kellyfarley/Desktop/oyez_pretty.json")
#task1 <- py_load_object("/Users/kellyfarley/Desktop/task1_data.pkl", pickle = "pickle")
#class0 <- py_load_object("/Users/kellyfarley/Desktop/class0.pkl", pickle = "pickle")
#class1 <- py_load_object("/Users/kellyfarley/Desktop/class1.pkl", pickle = "pickle")
raw <- read_csv(url("https://raw.githubusercontent.com/smitp415/CSCI_544_Final_Project/main/clean_data.csv")) # i think this is the best one to be using because it has the most information

dat <- raw
```

# Clean Data

```{r}
dim(dat)
```

We note 16 variables recorded for 3,303 SCOTUS cases. Only some of these variables are relevant for our analysis:

```{r}
clean <- dat %>%
  select(name, href, term) %>%
  mutate(term = case_when(term == "1789-1850" ~ round(mean(c(1789, 1850)), 0),
                   term == "1850-1900" ~ round(mean(c(1850, 1900)), 0),
                   term == "1900-1940" ~ round(mean(c(1900, 1940)), 0),
                   term == "1940-1955" ~ round(mean(c(1940, 1955)), 0),
                   TRUE ~ as.numeric(term))) %>%
  mutate(term = as.numeric(term))

load("/Users/kellyfarley/Desktop/mqData2020.Rda")

```

-term (numeric): Since 1955, the term of the Supreme Court begins from October of the given year and extends til October of the following year. Previously, the term of the court has been classified into broader terms: 1789-1850, 1850-1900, 1900-1940, 1940-1955. For our analysis, we want to consider "term" as a continuous variable. Therefore, the ranged values will be replaced with the year in the middle of the range. While it would be preferred to have the actual year of the decision, we note that very few cases take place before 1955: only 59 of the over 3k cases.
-first_party (character): This is the first party in the case name.
-second_party (character): This is the second party in the case name.
-facts: This is the raw text of the case facts and needs to be cleaned for future analysis.
-first_party_winner (logical): This variable indicates whether the first party won the case and is either TRUE or FALSE. We manipulate this variable to be equal to the name of the party who won the case.
-issue_area (factor): There are 14 possible issue areas a case can cover: "Attorneys," "Civil Rights," "Criminal Procedure," "Due Process," "Economic Activity," "Federal Taxation," "Federalism," "First Amendment," "Interstate Relations," "Judicial Power," "Miscellaneous," "Privacy," "Private Action," and "Unions." NA values are also possible.

We keep the "name" and "href" columns for ease of referral, though they will not be used for any statistical analysis.

https://www.supremecourt.gov/about/members_text.aspx justices on the court, could use as a proxy for conservative vs liberal, Democratic vs Republican appointed (may not hold up all of the time; president nominates and senate confirms; control of Senate)
https://en.wikipedia.org/wiki/Ideological_leanings_of_United_States_Supreme_Court_justices ideological leanings of court

who is the party? person or company?

model can be useful for controversial cases - don't always bring a case to the court because you want 1) a good set of facts, 2) a good composition of the court, because you don't want to set a bad precedent

take into account overall composition of court: # dem appointed, # rep appointed (would be even better to also know the Senate and know the individual justice's liberal vs conservative tendecies but prob not possible at this exact time)

https://mqscores.lsa.umich.edu/measures.php dataset of justice leanings 1937-2020 by justice with the year the justice serves...slight issue bc a few years have over 9 justices (times of switchover), but our model simply will just not deal with this... lol

# Data Structure

Dimensions
Variables
Years cases are from
Guilty / not guilty verdicts

# Most Frequent Words

Copy from Pset 1
Most frequent words across corpus, most frequent words across guilty versus not guilty, words with highest correlation

## Analysis

*What can this tell us about differences btwn guilty and not guilty verdicts?*

## Future Suggestions

# Verdict Prediction

Copy from Pset 2
Build test, training, prediction
Choose stopword, non-stopword, fighting words, Bayesian method

First, we break up our document term matrix into two separate dtm's: stop words (stop.dtm, for use in 1a) and non-stop words (non.dtm, for use in 1b). We use the stop words from the stopword package.

```{r}
# using stop words from stopword() package
stops <- stopwords(kind = "en")

# note apostrophes are not removed in stops, but are in our texts
stops <- c(gsub(stops, pattern="[']", replace=''))

# make corpus
texts=VCorpus(VectorSource(dat$facts))

# make dtm
dtm = as.matrix(DocumentTermMatrix(texts))

# remove non stop words

non.stops=array(TRUE,ncol(dtm))

stops=sort(stops)

col.names=colnames(dtm)
for(j in 1:length(stops)){
	ik=which(stops[j]==col.names)
	if(length(ik)>0){
		non.stops[ik]=F
	}
}

non.dtm  = dtm[,non.stops] # non-stop words
stop.dtm = dtm[,!non.stops] # stop words
```

Now, we work with the stop words dtm to make training (50% of known papers), testing (other 50% of known papers), and prediction (all disputed papers) sets. Note that we used our discretion on this training/testing proportion and have decided to split the known papers exactly in half due to overfitting and validation tradeoffs. We make this into the function splitSets() that can be used elsewhere in the pset and also, due to function scope, ensure that all variables needed later on (x.train, y.train, x.test, y.test, x.pred) are stored as global variables using the operator <<-.

```{r eval=F}
# function to split into testing, training, prediction sets
splitSets <- function(dtm){
  # define training, testing, and prediction sets
  
  # randomly assign known papers to training (50%) and testing (50%)
  N=nrow(dtm)
  s.vec=sample(1:2,replace=T,prob=c(1/2, 1/2),size=N)
  
  # train with 1/2 of known
  x.train <<- dtm[which(s.vec==1),]
  y.train <<- dtm$classes[which(s.vec==1)]
  
  # test with other 1/2 of known
  x.test <<- dtm[which(s.vec==2),]
  y.test <<- knownpapers$classes[which(s.vec==2)]
}

splitSets(stop.dtm)
```

## Analysis

*Are we able to predict guilty vs not guilty verdicts? What's the best method for prediction?*

## Future Suggestions